\documentclass[11pt,a4paper]{report}
\usepackage{sty-files/KAstyle}

\usepackage{fancyhdr}
\usepackage[noae,nogin]{Sweave}
\usepackage{natbib}           % bibliography
\usepackage{amsfonts,amsmath, amsthm} 
\usepackage{amsthm}           % to define new theorems
\usepackage{mdframed}         % for framed
\usepackage{float} % to put figures exactly Here
\usepackage[multidot]{grffile}  % to deal with figure names with 2 dots
\usepackage{multirow}
\usepackage{wrapfig}  % wrapping figures

% ----define new commands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


% a blankpage
\newcommand{\blankpage}{
\newpage
\thispagestyle{empty}
\mbox{}
\newpage
}


% ---- define new theorems
%\newtheorem{algo}{Pseudo algorithm}

  
% this is to define a new algo
  \newtheorem{mdtheorem3}{Pseudo algorithm}
  \newenvironment{algo}
{\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    { 
    \\ \hline
    \end{tabular} 
    \end{center}
    }



% ---Redefine Sweave Style -----------------

\setkeys{Gin}{width=0.8\textwidth} 

 %,height=4}

% Shorten the output lines that R produces


%1. Schunk
\renewenvironment{Schunk}
  {\vspace{2pt}\small}
  {\vspace{7pt}}

%2. Sinput
\DefineVerbatimEnvironment{Sinput}{Verbatim} {
  xleftmargin=10pt,
  fontshape=n, 
  frame=leftline, 
  framerule=2.5pt, 
  gobble=0, 
  framesep=8pt, 
  rulecolor=\color{bl2},
  formatcom=\color{bl1},
  fontsize=\footnotesize
 }

%3. Soutput
\DefineVerbatimEnvironment{Soutput}{Verbatim} {
  xleftmargin=10pt,
  fontshape=sl, 
  frame=leftline, 
  framerule=2.5pt, 
  gobble=0, 
  framesep=8pt,
  rulecolor=\color{gray},
  formatcom=\color{darkgray},
  fontsize=\footnotesize
}

% ------- colored background on title page ----
\usepackage{eso-pic}
\newcommand\BackgroundPic{%
\put(0,0){%
\parbox[b][\paperheight]{\paperwidth}{%
\vfill
%\vspace*{-4cm}
\centering
\includegraphics[width=1\paperwidth,height=1\paperheight, keepaspectratio]{Figures/Title/coverOneDaygris-MRN2018} \\      %  !!! cover title page
%\includegraphics[height=0.08\paperwidth, keepaspectratio]{Figures/Title/UOM}  \hspace{1cm} \includegraphics[height=0.08\paperwidth, keepaspectratio]{Figures/Title/cmri}   
\vfill
}}}


% ============================ start document ===========================
\begin{document}
\input{Figures/fig-concordance}
% no page numbering:
\pagenumbering{gobble}% Remove page numbers (and reset to 1)


% ---------------------------------------
% ---------- Title -----------
% ---------------------------------------

\AddToShipoutPicture*{\BackgroundPic}
\newpage\thispagestyle{empty}
~
\blankpage \thispagestyle{empty}


% -----------------------------------------------
%       Chapter: Foreword and acknowledgements
% ----------------------------------------------
\chapter*{Foreword}
% !Rnw root = mixOmics-material-oneday.Rnw



\subsection*{Objective}
During this workshop we will introduce fundamental concepts of multivariate dimention reduction, including the exploration and analysis of a single data set and the integrative analysis of multiple data sets. We will present statistical concepts such as covariance and correlation, multiple linear regression, classification and prediction, cross-validation, selection of diagnostic or prognostic markers, l$_1$ and l$_2$ penalties in a regression framework. \\
Each methodology that will be presented will be applied on biological `omics studies that include transcriptomics, metabolomics, proteomics, metagenomics data sets using our \R{mixOmics} R package, alternating theory and application on the case studies.


\subsection*{Requirements}
We expect the trainees to have a good working knowledge in R (e.g. handling data frames and perform simple calculations). Attendees are requested to bring their \textbf{own laptop}. % and their \textbf{own data they wish to analyse} during the last third day of the training. \\


To run the \R{R} scripts in this workshop, you \textit{may} need to install or update the \textit{latest} version of \textbf{R} available from the CRAN (currently > 3.5, \url{https://cran.r-project.org/}), followed by the update or installation of the \R{R} package \R{mixOmics} version 6.6.0 with one of the two following options: %See more details at \url{http://mixomics.org/2018/06/software-requirements-for-2018-mixomics-workshop/}. 

Installation from bioconductor:
\begin{Schunk}
\begin{Sinput}
> if (!requireNamespace("BiocManager", quietly = TRUE)) 
+   install.packages("BiocManager")
> BiocManager::install("mixOmics", version = "3.8")
\end{Sinput}
\end{Schunk}

Alternatively, you can install the development version of the package from \href{https://github.com/mixOmicsTeam/mixOmics}{Github}.
\begin{Schunk}
\begin{Sinput}
> install_github("mixOmicsTeam/mixOmics")
\end{Sinput}
\end{Schunk}
 
\textbf{For apple mac users}, if you are unable to install the \texttt{mixOmics} imported library \R{rgl} (\textit{rgl.so} error), you will \textbf{first} need to install the XQuartz software \url{https://www.xquartz.org/}. In addition we strongly advise using the \textbf{RStudio software} \url{http://www.rstudio.com/}. 


\subsection*{Versions of this workshop material}
This is the 10th version of this course material. The previous versions were:
\begin{enumerate}
\itemsep0em 
  \item One-day tutorial at the European Conference on Computational Biology 2014 (ECCB'14), Strasbourg, France, Sept 7 2014, 
  \item Two-day workshop hosted by the National Institute for Agricultural Research and organised by the Bioinformatics Facility Genotoul, Toulouse, October 6-7 2014, 
  \item Two-day workshop hosted and organised by the Statistical Consulting Centre, Department of Statistics, University of Auckland, New Zeland, 9-10 April 2015,
  \item Two-day workshop, Translational Research Institute, Brisbane Australia, Aug 13-14 2015,
  \item `French mixOmics workshop tour' taught at CIRAD Montpellier, Plateforme Biostatistique Genotoul, Institut Math\'ematiques de Toulouse and Plateforme Migale and INRA Jouy-en-Josas, Sept-Oct 2015, 
  \item Three-day workshops with a `\textit{bring your own data}' option in 2016 and 2017
  \item Two-day workshop in conjunction with the first Advanced mixOmics workshop, sponsored by Institut National Polytechnique Toulouse, and COST (European Cooperation in science and technology), INRA Toulouse, Oct and Nov 2017. 
  \item One-day workshop at Westmead Children's Medical Research Institute, Sydney, Australia, April 13 2018. 
  \item One-day workshop at the International Biometrics Conference, July 2018, and three-day workshop at the Univeristy of Melbourne, July 2018. 
\end{enumerate}
\noindent


\subsection*{Permission for reuse}
The course was written by Dr Kim-Anh L\^e Cao, with suggestions and constructive feedback from the mixOmics team and our attendees. Formal permission must be sought to redistribute, or reuse part of any material provided during this workshop. Acknowledgements will also be required if any of this work is quoted or cited. 



\subsection*{Acknowledgements}
The \R{mixOmics} core team (Drs Kim-Anh L\^e Cao, Florian Rohart, Ignacio Gonz\'alez and S\'ebastien D\'ejean) would like to thank theirformer students,  workshop participants and the key \R{mixOmics} developers \textbf{Dr Xin Yi Chua} (Queensland Facility for Advanced Bioinformatics, University of Queensland), \textbf{Mr Beno\^it Gautier} (The University of Queensland Diamantina Institute, Translational Research Institute), \textbf{Mr Francois Bartolo} (Institut de Math\'ematiques de Toulouse, Universit\'e Paul Sabatier) . They are also indebted to numerous \R{mixOmics} users who continuously help improving the package. \\
\noindent
We wish to acknowledge the funding bodies who made the mixOmics project possible: the National Health and Medical Research Council Career Development Fellowship (NHMRC, GNT1087\-415, KALC), the NHMRC Program grant (NHMRC, GNT1058993, BG) and Australian Research Council (ARC, DP130100777, FR), the Australian Cancer Research Foundation (ACRF) for the Diamantina Individualised Oncology Care Centre at The University of Queensland Diamantina (KALC, FR) and the Agence National de Recherche (ANR, SYNTHACS, FB).

%\newpage\thispagestyle{empty}

\section*{Your teacher}
\subsection*{Dr Kim-Anh L\^e Cao (core member of \texttt{mixOmics})}
%\begin{wrapfigure}[20]{L}{0.20\textwidth}
%\centering
\includegraphics[width=0.20\textwidth]{Figures/CV/180283_39_crop_small}
%\end{wrapfigure}
Dr Kim-Anh L\^e Cao (University of Melbourne, Australia) was awarded her PhD in 2008 at Universit\'e de Toulouse, France. She then moved to Australia as a postdoctoral fellow at the University of Queensland, Brisbane.  Since the beginning of her PhD Kim-Anh has initiated a wide range of valuable collaborative and research opportunities in both statistics and molecular biology. Her main research focus is on variable selection for biological data (`omics' data) coming from different functional levels by the means of multivariate dimension reduction approaches. Since 2009, her team has been working on developing a statistical software dedicated to the integrative analysis of `omics' data, to help researchers make sense of biological big data. Kim-Anh is a senior lecturer at the University of Melbourne (Melbourne Integrative Genomics, School of Mathematics and Statistics), and regularly runs statistical training workshops and short series seminars as well as mixOmics multi-day workshops. More details on Kim-Anh current research and collaborative projects: \url{http://lecao-lab.science.unimelb.edu.au/}.

During her spare time, Kim-Anh is a keen rock climber and bushwalker. Her favourite or common travel destinations are France and South Africa.


% \subsection*{S\'ebastien D\'ejean (core member of \texttt{mixOmics})}
% \begin{wrapfigure}{L}{0.15\textwidth}
% \centering
% \includegraphics[width=0.15\textwidth]{Figures/CV/SebastienDejean}
% \end{wrapfigure}
% 
% Dr S\'ebastien DÃ©jean was awarded his PhD in Applied Statistics in 2002 at Universit\'{e} de Toulouse, France after spending 4 years in a Biometry lab at INRA (French National Institute for Agronomic Research). Since then he has been working at the Toulouse Mathematics Institute (Universit\'{e} de Toulouse, France) as a research engineer. S\'ebastien likes to interact closely with scientists working in different research areas ranging from high-throughput biology, chemistry to information retrieval. He is an expert in statistical data analysis and he contributes to the development of several \texttt{R} packages including \texttt{mixOmics}.\\
% 
% S\'ebastien is deeply committed into teaching, including training statistical/software workshops for researchers and scientific and administrative staff. He can illustrate Principal Component Analysis with a fishing rod, a pizza-box and a soccer ball; do you know why?\\
% 
% Outside work, S\'ebastien runs! From 10km (best performance 36'31 as of June 2018) to marathons (3h26', to be improved). He runs with the green and yellow team jersey of the Athletics Coaching Club Ramonville. S\'ebastien was qualified for the the 10km French championship in his year category in Aubagne in 2017. He'll start running marathons again in October this year!
% 
% 



% ---------------------------------------
% ---------- Table of content -----------
% ---------------------------------------
\tableofcontents

\setcounter{page}{1}

% -------------------
%\setcounter{page}{1}
\clearpage
\pagenumbering{arabic}% Arabic page numbers (and reset to 1)
% -------------------


% % -----------------------------------------------
% %       Chapter: introduction
% % ----------------------------------------------
%\part{Introduction}\label{Intro}
\chapter{Introduction}\label{Intro}
% !Rnw root = mixOmics-material-oneday.Rnw


\section{What is \texttt{mixOmics}?}
\R{mixOmics} is a freely available R package currently implementing nineteen methodologies for the exploration and the integration of biological data sets. Note that that mixOmics is not limited to biological data only and can be applied to other type of data where integration is required. A strong focus is given to graphical representation to better understand the relationships between omics data and visualize the correlation structure at the sample and variable levels. \\

Besides the \R{R} package, the \R{mixOmics} project includes:
\begin{itemize} \itemsep0em 
  \item a website with extensive tutorials: \url{http://www.mixOmics.org}
  %\item a Beta version of a web-interface: \url{http://mixomics.qfab.org/}
  %\item a beta version of a shiny web-interface (currently: single data set analysis): \\
  % \url{http://mixomics-projects.di.uq.edu.au/Shiny/}
\end{itemize}

\noindent
\textbf{Want to be part of the mixOmics community?}
Subscribe to our newsletter mailing list by simply sending an e-mail with no subject of body to: \\
\url{mixomics-news-subscribe@math.univ-toulouse.fr} \footnote{You can unsubscribe at any time by sending an e-mail to {\footnotesize \url{mixomics@math.univ-toulouse.fr}}}. \\
Follow us on \includegraphics[width=0.02\paperwidth]{Figures/Title/Twitter}: \textcolor{blue}{@ mixOmics\_team}

\noindent
\textbf{Any question or feedback?} If you experience a bug, you can notify us on \url{https://github.com/mixOmicsTeam/mixOmics/issues}. For other questions, emails us at \url{mixomics@math.univ-toulouse.fr}.

\section{Why multivariate methods?}
Single `omics data analysis does not provide enough information to give a deep understanding of a biological system. We can obtain a more precise picture of a system by combining multiple `omics data sets. In \R{mixOmics}  we propose a whole range of multivariate methods that we developed and validated on many biological studies. \\
Multivariate methods are well suited to large `omics data sets where the number of variables (e.g. genes, proteins, metabolites) is much larger than the number of samples (patients, cells, mice). They have the appealing properties of reducing the dimension of the data by using instrumental variables (`components'), which are defined as combination of all variables. Those components are then used to produce useful graphical outputs that enable better understanding of  the relationships and correlation structure between the different data sets that are integrated. We have further developed sparse multivariate models to identify the key variables that are highly correlated, or explain the biological outcome of interest. The identified variables are then more amenable to `classical' univariate statistical inference and the generation of novel biological hypotheses.


\section{\texttt{mixOmics} philosophy}
The multivariate statistical methods implemented in \R{mixOmics} aim at summarizing the main characteristics of the data while capturing the largest sources of variation in the data. There is an underlying stastistical model for each method, as detailed in the methods section, however those models radically differ from univariate formulations as they do not test one variable at a time, or produce p-values! In that sense, multivariate methods are mostly considered as `exploratory' methods as they do not enable statistical inference, however, the \textit{biological question matters} in order to apply the suitable multivariate method, as we will emphasize during the workshop. \\
The graphical outputs resulting from projecting the data into a much smaller subspace than the original data space enable to:
\begin{itemize}
  \item assess if the separation of the samples is in agreement with a phenotype of interest (discrete or continuous outcome),
  \item identify potential sample outliers,
  \item reveal the effect of confounding variables, laboratory or platform effects.
\end{itemize}
In addition, a strong focus is given to statistical data integration on matching data sets (i.e. experiments performed on the same individual or samples), where the aim is to maximise the common information between those data sets.  The novel integrative multivariate methodologies that we propose were specifically developed for large data sets and the sparse approaches in particular allow for the identification of key features amongst the thousands that are measured.



\begin{figure}[!h]
\begin{center}
\includegraphics[angle=0,width=1\textwidth]{Figures/Intro/framework-global}
\end{center}
\caption{Global framework of \R{mixOmics} methods. \label{fig:frameworkV6}}
\end{figure}



\section{How do I cite \texttt{mixOmics}?}
Each method has an associated methods paper (see below). We also recently published a  generic software article \citep{mixOmics}, which is referred as follows when you type in R: \R{citation("mixOmics")}\!: \\
Rohart F, Gautier B, Singh A, L\^e Cao K-A (2017). mixOmics: an R package for 'omics feature selection and multiple data integration. \href{http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005752}{\textit{PLoS Computational Biology} 13(11)}. \\

% \noindent The manual R version (type in R: \R{citation("mixOmics")}), for example (\citealt{mixOmicsR}): \\
% $\bullet$ L\^e Cao K-A, Rohart F, Gonz\'alez I,  and D\'ejean S with key contributors Gautier B, Bartolo F and contributions from Monget P,  Coquery J, Yao F and Liquet B (2017). mixOmics: Omics Data Integration Project. \textit{R package version 6.4.2}. %, \url{{https://CRAN.R-project.org/package=mixOmics}}.


\section{Flexibility of \texttt{mixOmics}}
The package \R{mixOmics} offers a wide range of multivariate methodologies (Figure \ref{fig:frameworkV6}) to address different types of biological questions or guide further analytical investigations which we list below. Several topics will not be presented during this one-day workshop but are listed in Section \ref{notCovered}.



\subsection*{Exploring one single data set (e.g. transcriptomics data set)}

\begin{itemize}
  \item {\color{blue}Principal Component Analysis (PCA)} enables to identify trends or patterns in the data and to understand if the major sources of variation come from experimental bias, batch effects or observed biological differences. The sample plots enable to visualise if the biological samples `naturally' cluster according to the biological conditions. More details about the PCA approach can be found in \citet{Jol05}, see Chapter \ref{method:PCA}. 
  
  \item {\color{blue}sparse Principal Component Analysis (sPCA)} \citep{She07} enables the selection of features (e.g. transcripts) that contribute the most to the variation highlighted by PCA in the data set, see Chapter \ref{method:PCA}. 
\end{itemize}



\subsection*{Classifying different groups of samples according to a discrete outcome}
In that specific setting, $X$ = expression data and $Y$ is a vector indicating the class of each sample.

\begin{itemize}
  \item {\color{blue}PLS-Discriminant Analysis (PLS-DA)} is a linear model that performs a classification task and predicts the class of new samples \citep{Bar03}, see Chapter \ref{method:PLSDA}. 
  \item The sparse variant {\color{blue}sparse PLS-DA (sPLS-DA)} \citet{Lec11} selects the most predictive / discriminative features in the data that help classifying the samples, see Chapter \ref{method:PLSDA}. 
\end{itemize}

\subsection*{Multivariate analysis of microbial communities}
The \R{mixMC} framework has been specifically developed for microbiome data analysis, and in particular 16S experiments. Challenges in microbiome data include sparse counts and uneven sequencing depth that require to convert counts into relative proportions, leading to \textit{compositional data}. This poses a problem in most statistical analyses performed on compositional data are not suitable for relative proportions, resulting in spurious results. We  use log ratio transformations, as proposed by \citet{Ait82} which is implemented in PCA and PLS methods \citep{Lec16}. We provide a case study in this workshop (Chapter \ref{study:mixMC}), see also \url{http://mixomics.org/mixmc/} for more examples.


% \subsection*{$N-$ integration: integrating more than two data sets measured on the same samples (see Chapter \ref{study:multiblock})}
% The \textit{multiblock analysis} module implements an improved version of Generalised Canonical Correlation Analysis (GCCA \citealt{Ten14}) for the integration of more than two data sets (Fig. \ref{fig:frameworkV6}). Three variants are currently proposed: regularized GCCA (a generalization of rCCA) to maximise correlation between all data sets, sparse GCCA (a generalization of sPLS) to select correlated variables from each data set, and sparse GCC Discriminant Analysis (sGCC-DA, a generalization of sPLS-DA) to select correlated and discriminative variables in a supervised context (see case study Chapter \ref{study:multiblock} for our new \R{DIABLO} framework, \citealt{Sin16}). We are currently adding more functionalities to this module, contact us for more details, see also \url{http://mixomics.org/mixdiablo/}.
% 


% \begin{figure}[!h]
% \begin{center}
% \includegraphics[angle=0,width=0.8\textwidth]{Figures/Intro/framework-oneday}
% \end{center}
% \caption{\R{mixOmics} methods presented during this one-day workshop with different variants for variable selection \label{fig:framework-oneday}}
% \end{figure}


\section{Ongoing methodological developments}
\subsection*{Longitudinal or time-course multi-omics data modelling and analysis (not covered during this workshop)}
We are currently working on linear mixed spline models to be integrated into \R{mixOmics}. Contact us for more details (manuscript and package \texttt{lmms} are available for the first level of analysis, \citealt{Str15}).


% \section{Summary of the different case studies used in this workshop}
% Several data sets are available in the \texttt{mixOmics} library and will be used as case studies throughout the workshop. We summarise their main characteristics in Table \ref{tab:cs}.
% 
% 
% \begin{center} 
% \begin{table}[H]
% \caption{Summary of the different case studies analysed during the workshop. }\label{tab:cs}
% \scalebox{0.7}{
% \begin{tabular}{l l l l l l} \hline 
% Name & Type of data & \# of samples & Treatment/Group & Methods & More details \\ \hline 
% 
% \multirow{3}{*}{multidrug} & ABC transporters (48)    & 60 cell lines & cell lines (9)  & PCA, sPCA    & \R{?multidrug} \\ 
%                             & gene expr. (1,429) &               &                 & NIPALS    &  Analysed in Chap \ref{study:MultidrugPCA}\\ \hline 
% %                            &                         &               &                 & IPCA, sIPCA   &  \\ \hline  
%                             
% % \multirow{2}{*}{nutrimouse} & lipids (21) & 40 mice & diet (5)        & rCCA    & \R{?nutrimouse} \\ 
% %                             & gene expr. (120) &  & genotype (2) &     &  Analysed in Chap \ref{study:nutrimouse}\\   \hline                             
% %         
% %                             
% % \multirow{3}{*}{liver.toxicity} & gene expr.  (3,116)    & 64 rats       & dose (4)          & sPLS    & \R{?liver.toxicity} \\ 
% %                                 & clinical measures (10)     &               & time necropsy (4) & multilevel sPLS    &  Analysed in Chap \ref{study:liver}\\  
% %                                 &                            &               &                  &   &  \\ \hline                             
%                             
%   
% \multirow{3}{*}{srbct} & gene expr.  (2,308)    & 63 human tumour       & tumour type (4)          & PCA    & \R{?srbct} \\ 
%                                 &    & samples               &  &  PLS-DA, sPLS-DA  &  Analysed in Chap \ref{study:SRBCT}\\  
%                                 &                            &               &                  &   &  \\ \hline   
%                                 
% % \multirow{3}{*}{vac18} & gene expr.  (1,000)    & 42 PBMC human        & stimulation (4)          & multilevel PCA     & \R{?vac18} \\ 
% %                                                     &    &samples              & &  multilevel sPLS-DA   &  Analysed in Chap \ref{study:vac18}\\  
% %                                 &                            &               &                  &   &  \\ \hline        
% 
% % \multirow{3}{*}{vac18.simulated} & gene expr.  (500)    & 48 simulated samples & stimulation (4) & multilevel sPLS-DA    & \R{?vac18.simulated} \\ 
% %                                 &    &               & time (2)    &    &  Analysed in Chap \ref{study:vac18} \\  
% %                                 &                            &               &                  &   &  \\ \hline        
%  \multirow{2}{*}{diverse.16S}  & OTU (1674) & 162 samples & bodysites (3)        & multilevel PCA  & \R{?diverse.16S} \\ 
%                            &  &  &  & multilevel sPLS-DA  &  Analysed in Chap \ref{study:mixMC} \\  \hline
% \multirow{3}{*}{breast.TCGA Training } & mRNA (200) & 150 tumour samples & tumour subtype (3)        & \R{block.splsda}    & \R{?breast.TCGA} \\ 
%                            & miRNA (184) &  & genotype (2) & (sGCC-DA)     &  Analysed in Chap \ref{study:multiblock}\\  
%                             & protein (142)                      &  &          &    &  \\ \hline 
%                             
% \multirow{2}{*}{breast.TCGA Validation}  & mRNA (200) & 70 tumour samples & tumour subtype (3)        & \R{predict.splsda}    & \R{?breast.TCGA} \\ 
%                           & miRNA (184) &  & genotype (2) & (sGCC-DA)    &  Analysed in Chap \ref{study:multiblock} \\  \hline
% 
% % \multirow{2}{*}{diverse.16S}  & OTU (1674) & 162 samples & bodysites (3)        & multilevel PCA  & \R{?diverse.16S} \\ 
% %                           &  &  &  & multilevel sPLS-DA  &  Analysed in Chap \ref{study:mixMC} \\  \hline
%                     
%                   
% \end{tabular} 
% }
% \end{table}
% \end{center}


\section{Input data in \texttt{mixOmics}}

\subsection*{Data format}
The input data should be numerical data frames or matrices of size $n \times p$, where $n$ is the number of samples or observational units (individuals) and $p$ is the number of biological features or variables. Each matrix should represent one type of `omics or biological feature (not necessarily 'omics!). In the case of integrative analysis, each data set should be \textit{sample matched}, \textit{i.e.} row 1 in data set $X$ and in data set $Z$ should correspond to the same individual. \\
The data will be input differently depending on the type of method that is used. Analytical methods for one data set (PCA, PLS-DA) take the input matrix as the argument \argu{X = \!\!}, while methods to integrate  data sets (two data sets: PLS, RCCA, more than two data sets: RGCCA, SGCCA and SGCC-DA) take the input matrices as distinct separate arguments (\argu{X = \!\!} and \argu{Y = \!\!}) either as matrices for the integration of two data sets, or a list of matrices for more than two data sets. %. Integrative methods for more than two data sets (RGCCA, SGCCA and SGCC-DA) take as input data a \underline{list of data sets} in the argument \argu{X = \!\!}).


\subsection*{Normalisation is not part of the package}
In \R{mixOmics} we assume the input data to be \textit{normalised} using appropriate techniques specific for the type of `omics technology platform. Our methods can handle molecular features measured on a continuous scale (e.g. microarray, mass spectrometry-based proteomics and metabolomics) or sequenced-based count data (RNA-seq, 16S, shotgun metagenomics) that become `continuous' data after pre-processing and normalisation. 

For microbiome 16S data, we assume that the OTU count data are pseudo counts (i.e. similar to RNA-sequencing data, we set data = data + 1) before they are normalised using Total Sum Scaling normalisation, see details here: \url{http://mixomics.org/mixmc}.

\subsection*{Filtering variables}
While \texttt{mixOmics} methods can handle large data sets (several tens of thousands of predictors), we recommend pre-filtering the data to less than 10K predictors per data set, for example by using Median Absolute Deviation \citep{Ten16} for RNA-seq data, by removing consistently low counts in microbiome data sets \citep{Aru11, Lec16} or by removing near zero variance predictors. Such step aims to lessen the computational time during the parameter tuning process.

\subsection*{Centering and scaling the data}
By default the PCA method will center and scale each variable unless set to \argu{FALSE} in the arguments \argu{center} and \argu{scale\!\!}. \\
All PLS related methods which maximises the covariance (PLS, PLS-DA, RGCCA and SGCCA) center and scale \textit{by} each variable so that each variable has a mean 0 and a variance 1 (argument \argu{scale}). \\
The RCC method maximises the correlation of the data, and therefore does not center nor scale the data. 

\section*{Summary of classical \texttt{mixOmics} functions}


\begin{figure}[!h]
\begin{center}
\includegraphics[angle=0,width=1\textwidth]{Figures/Intro/cheatsheet-classic}
\end{center}
\caption{Feeling lost? Cheatsheet of the main mixOmics methods, parameters and associated functions.  \label{fig:cheatsheet}}
\end{figure}




\section{Other analyses not presented during this workshop}\label{notCovered}
%\paragraph{Estimating missing values in the data}
%Most of the multivariate methods in mixOmics can be performed with missing values. However, there are some cases where estimating missing values prior would be beneficial, as long as the proportion of missing values is $< 20\%$. The non-linear iterative partial least squares algorithm (\textcolor{blue}{NIPALS}, \citealt{Wol66}) forms the basis on PLS regression and performs iterative principal component analysis. The local regressions performed in the algorithm enable to estimate missing values, see Chapter \ref{method:PCA} and \citealt{Ten98} Chap. 6 for our French readers.

\paragraph{Analysing repeated measurement or a cross-over design}
When different treatments are applied on the same subjects or samples, a multilevel approach is required to highlight subtle differences within individuals but between treatments against the large individual variation between individuals \citep{Liq12}. A \R{multilevel} argument is available in the mixOmics methods PCA, PLS-DA and PLS for this specific experimental design. Usually this type of design includes a very small number of time points or repeats (2 to 3).
% 
% \paragraph{Multivariate analysis of microbial communities}
% The \R{mixMC} framework has been specifically developed for microbiome data analysis, and in particular 16S experiments. Challenges in microbiome data include sparse counts and uneven sequencing depth that require to convert counts into relative proportions, leading to \textit{compositional data}. This poses a problem in most statistical analyses performed on compositional data are not suitable for relative proportions, resulting in spurious results. We  use log ratio transformations, as proposed by \citet{Ait82} which is implemented in PCA and PLS methods \citep{Lec16}. We provide a short example in this workshop, see also \url{http://mixomics.org/mixmc/} for more examples.

\paragraph{Integrating two data sets measured on the same individuals/samples (e.g. transcriptomics and proteomics data)}
\begin{itemize}
  \item To extract or highlight common information from two matching data sets:
    \begin{itemize}
      \item {\color{blue}Canonical Correlation Analysis (CCA)} \citep{Gon08} or {\color{blue}Partial Least Squares (PLS) canonical mode} when the total number of features is less than the number of samples. 
      \item {\color{blue}regularized Canonical Correlation Analysis (rCCA)} \citep{Gon09} or {\color{blue}Partial Least Squares (PLS) canonical mode} when the total number of features is greater than the number of samples.
    \end{itemize}
\item To model a linear relationship between multiple \textit{continuous} responses in the $Y$ data set with multiple predictors (in the $X$ data set) we use  {\color{blue}Partial Least Squares (PLS)}, classic or regression mode. Example: to model or predict the expression of metabolites  in $Y$ given the expression of transcripts in $X$.
    \item To select features (genes, proteins) from both data sets that \textit{covary} (i.e. `change together', or are `co-expressed') across all samples, in an \textit{unsupervised} framework, we use {\color{blue}sparse Partial Least Squares (sPLS)} with a suitable mode \citep{Lec08, Lec09a}.
\end{itemize}

\paragraph*{$N-$ integration: integrating more than two data sets measured on the same samples}
The \textit{multiblock analysis} module implements an improved version of Generalised Canonical Correlation Analysis (GCCA \citealt{Ten14}) for the integration of more than two data sets (Fig. \ref{fig:frameworkV6}). Three variants are currently proposed: regularized GCCA (a generalization of rCCA) to maximise correlation between all data sets, sparse GCCA (a generalization of sPLS) to select correlated variables from each data set, and sparse GCC Discriminant Analysis (sGCC-DA, a generalization of sPLS-DA) to select correlated and discriminative variables in a supervised context (see case study on our website  \url{http://mixomics.org/mixdiablo/} with our  \R{DIABLO} framework, \citealt{Sin16}). We are currently adding more functionalities to this module.
% 


\paragraph{$P-$ integration: integrating studies measured on the same variables}
The Multivariate INTegrative method \R{MINT} extends multi-group analysis from \citep{Esl14} to combine different studies generated from different labs and technological platforms (Fig. \ref{fig:frameworkV6}). This is a great challenge as for data generated under similar biological conditions, the difference between these series of measurements is so large that it acts as a confounding factor in the combined analysis. This may lead to spurious relationships between the biological outcome of interest and the effect of the different technological platforms. This systematic error is often generally referred to as `batch effect' \citep{Roh16}. We are currently adding more functionalities to this module, contact us for more details, see also \url{http://mixomics.org/mixmint/}.








% % % -----------------------------------------------
% % %       Chapter: methods
% % % ----------------------------------------------

%PCA
\chapter{Principal Component Analysis}\label{method:PCA}
% !Rnw root = mixOmics-material-oneday.Rnw

% ---Redefine Sweave Style -----------------

\setkeys{Gin}{width=0.6\textwidth} 

 %,height=4}



% ----------------- SECTION -------------------------------------
\section{Why use PCA?}
Principal Component Analysis \citep{Jol05} is primarily used to explore \textbf{one single type of `omics data} (e.g. transcriptomics, proteomics, metabolomics, etc) and identify the largest sources of variation. PCA is an \textit{unsupervised} technique, which means that no information about the class of the samples/patients is taken into account in the method itself. PCA is an extremely useful visualisation technique to identify expression patterns that may arise due to biological variation or systematic bias. Smilarities and differences can be observed at the samples level given their variables measurements. We often use \textbf{PCA as a preliminary step} to better understand the sources of variation of the data. \\

Variants of PCA have been proposed to perform variable selection by using different ways of penalising the variable weights vectors in PCA (`sparse PCA', see for example \citealt{She07}).

\medskip
\begin{mdframed}%[backgroundcolor=my-light-gray]
\begin{itemize}
  \item {\color{blue}Principal Component Analysis (PCA)} visualises trends or patterns in the data and enables us to understand whether the major sources of variation come from experimental bias, batch effects or observed biological differences. Sample plots enable to visualise sample clusters, and check whether those clusters agree with some biological variation introduced in the experiment (e.g. treatment). 
  
  \item {\color{blue}sparse Principal Component Analysis (sPCA)}  enables the selection (identification) of variables that contribute the most to the variation highlighted by PCA in the data set.
  
  \item {\color{blue}Estimation of missing values.} The non-linear iterative partial least squares algorithm (\textcolor{blue}{NIPALS}) performs iterative principal component analysis and the local regressions in the algorithm allow for the estimation of missing values.
\end{itemize}
\end{mdframed}
\medskip


% -------------- SECTION: Principle ------------------------
\section{Principle}

\subsection{Principal Component Analysis}
The aim of PCA \citep{Jol05}  is to reduce the dimensionality of the data while retaining as much information as possible. `Information' is referred here as \textit{variance}. The idea is to create uncorrelated artificial variables called \textbf{principal components} (PCs) that combine in a linear manner the original (possibly correlated) variables (e.g. genes or metabolites). \\
Dimension reduction is achieved by projecting the data into the space spanned by the principal components. In practice, it means that each sample is assigned a score on each new PC dimension - this score is calculated as a linear combination of the original variables to which a weight is applied. The weights of each of the original variables are stored in the so-called \textbf{loading vectors} associated to each Principal Component (see Fig. \ref{framework:PCA}). \\


\begin{figure}[!h]
\begin{center}
\includegraphics[angle=0,width=300pt]{Figures/Methods/PCA/PCA-framework} 
\end{center}
\caption{Schematic view of the PCA matrix decomposition into a set of principal component vectors and loading vectors.}\label{framework:PCA}
\end{figure}


The dimension of the data is reduced by projecting the data into the smaller subspace spanned by the PCs, while capturing the largest sources of variation between samples. \\

The principal components are obtained by \textbf{maximising  the variance-covariance matrix} of the data. To that end, we calculate the eigenvectors/eigenvalues of the variance-covariance matrix, often via singular value decomposition when the number of variables is very large. The data are usually centered (\argu{center = TRUE}), and sometimes scaled (\argu{scale = TRUE}) in the method. The latter is especially advised in the case where the variance is not homogeneous across variables. \\

\textbf{Missing values} are not allowed in the classical PCA algorithm but the use of the NIPALS algorithm (nonlinear iterative partial least squares \citet{Wol87} can handle missing values as well as estimate those missing values (see Section \ref{NIPALS}).\\

The first PC is defined as the linear combination of the original variables that explains the greatest amount of variation. The second PC is then defined as the linear combination of the original variables that accounts for the greatest amount of the \textit{remaining variation} subject of being orthogonal (uncorrelated) to the first component. Subsequent components are defined likewise for the other PCA dimensions. The user must therefore report how much information is explained by the first PCs as these are used to graphically represent the PCA outputs. 

\subsection{Sparse Principal Component Analysis}\label{meth:spca}
In \R{mixOmics} we have implemented sPCA \citep{She07} which is based on singular value decomposition and is appropriate to deal with large data sets. `Sparsity' is achieved via LASSO penalisations (see Section \ref{math:PCA} for more details).\\

When applying sparse PCA, the orthogonality of the principal components and of the loading vectors is often lost. %We used the method of \citep{Wit09}%\textcolor{red}{Witten et al. (2009), did we really do that??} 
%to force orthogonality among PCs. 
Our experience has shown that setting \R{scale = TRUE} helps in obtaining orthogonal sparse loading vectors. The method is not classically used, but serves as a good introduction to the use of LASSO penalties in our methods, see Remark in \ref{tuning:sPCA}.

\subsection{Non-linear Iterative PArtial Least Squares algorithm (NIPALS)}\label{NIPALS}
NIPALS can handle missing values without having to remove either samples or variables by the means of local least squares regression. This algorithm proposed by \citet{Wol66} was at the premise of Partial Least Squares regression. % (see Section \ref{method:PLS}). 
NIPALS is an iterative way of solving PCA. When there are no missing values, the two approaches give the same results. \\
\paragraph{When do we use NIPALS?} 
\begin{itemize}
\item When a classical PCA on a data set with missing values yields to a barplot where the proportion of variance does not decrease with the number of components! (using \R{plot(pca.result)}). This indicates that there are too many missing values. \\
\item Beware that the algorithm tends to give good estimates when the number of missing values is low ( < 20\% of the whole data set).
\end{itemize}

% ------------------SECTION: Parameter tuning -----------------------
\section{Choosing the optimal parameters}
\subsection{Number of Principal Components}
We can obtain as many dimensions (i.e. number of PCs) as the rank of the matrix (i.e. the number of independent or uncorrelated variables). In mixOmics we set to the minimum between the number of samples and variables. However, the goal is to reduce the complexity of the data and therefore summarize the data in fewer underlying dimension. The number of Principal Components to retain (also called the number of dimensions) is therefore crucial when performing PCA. \\

Visualising the eigenvalues associated with each PC (i.e. the amount of variance explained per component) via a barplot is an empirical way of choosing the number of PCs to retain in the analysis. We choose the minimal number of PCs when the decrease of the eigenvalues appears to stabilise (i.e. when the `elbow' appears). \\

Figure \ref{pca.screeplot} gives an example of such screeplot which suggests that 3 components 
might be satisfactory to explain (and visualise) most of the variance from the data. A useful output is the cumulative percentage of explained variance (or information). In the example \ref{usage:PCA}, two PCs explained 57.48\% of the total variance, and three PCs explain 74\% of the total variance. Note that the percentage of explained variance is highly dependent on the size of the data and the correlation structure between variables. \\

Another criterion is the clarity of the final graphical output. These criteria are highly subjective and the user must keep in mind that visualization becomes difficult above 3 dimensions. \\

We have only discussed empirical criteria to choose the number of PCs. There exists some theoretical criteria, however, they are often limited to gaussian distributions and when the data are not scaled. For scaled data, the Kaiser criterion advises to only retain principal components with an eigenvalue $>1$, but how absolute is that threshold? Other authors have attempted to refine that threshold (see \citealt{Kar03}). %It hsould be kept in mind however that this threshold should not be considered as a fixed absolute value.


\subsection{sparse PCA: number of variables to select}\label{tuning:sPCA}
For sparse PCA, the number of variables to select on each PC must be input by the user (\argu{keepX}).  
In \R{mixOmics} we do not propose a tuning criterion to tune \argu{keepX} in sPCA. We have observed that the proportion of explained variance significantly drops compared to PCA, which is to be expected. Consequently the amount of explained variance is not a suitable tuning criterion.\\

Since sPCA is an unsupervised and exploratory technique, we prefer to let the user select a (\argu{keepX}) suitable to the research question. The sparse methodology is useful to remove some of the non informative variables in PCA and can be used to investigate whether `tighter' sample clusters can be obtained and which are the variables that highly contribute to the definition of each PC.

\textit{Remark.} sPCA is not classically used in mixOmics because of the difficulty to choose the sparsity penalties. We favour instead a classical PCA with a correlation circle plot (\R{plotVar} function) with a set cutoff (argument \argu{cutoff}) to visualise the variables that seem to drive the variation in the data. 

\subsection{NIPALS: number of principal components}
To ensure a good estimation of the missing values NIPALS requires a large number of dimensions (>10). Currently \R{mixOmics} does not propose any criterion to tune the number of principal components but the user should try to retain as much explained variance as possible to enable the estimation of missing values (see details in Section \ref{math:NIPALS}). Note that since this algorithm is iterative, the computation time will increase with the size of the data set.

% {\bf Principal Component Analysis} (PCA) is a mathematical procedure that uses orthogonal linear transformation of data from possibly correlated variables into uncorrelated principal components (PCs). The first principal component explains as much of the variability in the data as possible, and each following PC explains as much of the remaining variability as possible. Only the PCs which explain the most variance are retained. This is why choosing the number of dimensions or component (ncomp) is crucial.
% 
% Input data should be centered ({\tt center = TRUE}) and possibly (sometimes preferably) scaled so that all variables have a unit variance. This is especially advised in the case where the variance is not homogeneous across variables ({\tt scale = TRUE}).



% ---------------- SECTION: usage -----------------
\section{Usage in mixOmics}
Load \R{mixOmics}:
\begin{Schunk}
\begin{Sinput}
> library(mixOmics)
\end{Sinput}
\end{Schunk}


\subsection{PCA}\label{usage:PCA}
\paragraph{Load the data.}

Input data should be centered \texttt{(center = TRUE)} and sometimes preferably scaled \texttt{(scale = TRUE)} so that all variables have a unit variance. This is especially advised in the case where the variance is not homogeneous across variables. By default, the variables are centered but not scaled in our function, but the user is free to choose other options.

The input data matrix $X$ is of size $n$ samples (in rows) and $p$ variables (here genes) in columns. We use the liver toxicity gene expression data set that contains the expression measure of 3116 genes for 64 rats that were exposed to non-toxic, moderately toxic or severely toxic doses of acetaminophen in a controlled experiment, see \texttt{?liver.toxicity}:

\begin{Schunk}
\begin{Sinput}
> data(liver.toxicity)
> X <- liver.toxicity$gene
> dim(X)
\end{Sinput}
\begin{Soutput}
[1]   64 3116
\end{Soutput}
\end{Schunk}


\paragraph{Choosing the number of components.}

We can obtain as many dimensions (i.e. number of PCs) as the rank of the data matrix (often between $n$ the number of samples, and $p$ the number of variables. However, as the goal of PCA is  to reduce the complexity of the data, we will summarize the  data in fewer  \textbf{dimensions}, or PCs.

The function  \R{tune.pca()} will plot the barplot of the proportion of explained variance for each PC, here for example for the first 10 PCs: 

\begin{Schunk}
\begin{Sinput}
> tune.pca(X, ncomp = 10, center = TRUE, scale = FALSE)
\end{Sinput}
\end{Schunk}

\setkeys{Gin}{width=0.6\textwidth}
\begin{figure}[!hb]
\begin{center}
\begin{Schunk}
\begin{Soutput}
Eigenvalues for the first 10 principal components, see object$sdev^2: 
       PC1        PC2        PC3        PC4        PC5 
17.9714164  9.0792340  4.5677094  3.2043829  1.9567988 
       PC6        PC7        PC8        PC9       PC10 
 1.4686086  1.3281206  1.0820554  0.8434155  0.6373565 

Proportion of explained variance for the first 10 principal components, see object$explained_variance: 
       PC1        PC2        PC3        PC4        PC5 
0.35684128 0.18027769 0.09069665 0.06362638 0.03885429 
       PC6        PC7        PC8        PC9       PC10 
0.02916076 0.02637122 0.02148534 0.01674690 0.01265538 

Cumulative proportion explained variance for the first 10 principal components, see object$cum.var: 
      PC1       PC2       PC3       PC4       PC5       PC6 
0.3568413 0.5371190 0.6278156 0.6914420 0.7302963 0.7594570 
      PC7       PC8       PC9      PC10 
0.7858283 0.8073136 0.8240605 0.8367159 

 Other available components: 
 -------------------- 
 loading vectors: see object$rotation 
\end{Soutput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-example_tune_pca_plot}
\end{center}
\caption{Example of PCA screeplot of the proportion of explained variance on each PC on the Liver Toxicity data.}\label{pca.screeplot}
\end{figure}

The rule of thumb to choose the number of component is not so much to set a hard threshold based on the cumulative proportion of explained variance  (as this is data-dependent), but to observe when a drop or elbow, appears on the barplot output above, as this indicates that the remaining variance is spread over many PCs and is not of relevance to obtain a low dimensional `snapshot' of the data. In this specific example we could choose between 2 to 3 components for the final PCA. 


\paragraph{PCA.}

Based on the preliminary analysis above, we run a PCA with 3 components:
\begin{Schunk}
\begin{Sinput}
> pca.result <- pca(X, ncomp = 3, center = TRUE, scale = FALSE)
\end{Sinput}
\end{Schunk}

The output is the square eigenvalues from the singular value decomposition (equal to the amount of variance explained) and related outputs such as the proportion of explained variance relative to the total amount of variance in the data, or the cumulative proportion of explained variance when we add more components:
\begin{Schunk}
\begin{Sinput}
> pca.result
\end{Sinput}
\begin{Soutput}
Eigenvalues for the first 3 principal components, see object$sdev^2: 
      PC1       PC2       PC3 
17.971416  9.079234  4.567709 

Proportion of explained variance for the first 3 principal components, see object$explained_variance: 
       PC1        PC2        PC3 
0.35684128 0.18027769 0.09069665 

Cumulative proportion explained variance for the first 3 principal components, see object$cum.var: 
      PC1       PC2       PC3 
0.3568413 0.5371190 0.6278156 

 Other available components: 
 -------------------- 
 loading vectors: see object$rotation 
\end{Soutput}
\begin{Sinput}
> plot(pca.result)
\end{Sinput}
\end{Schunk}

Here the total variance is
\begin{Schunk}
\begin{Sinput}
> pca.result$var.tot
\end{Sinput}
\begin{Soutput}
[1] 50.36249
\end{Soutput}
\end{Schunk}
and a PCA with 3 components explains 62.78 of the total variance (we sum the eigenvalues squared and divide by the total amount of variance with:  \texttt{sum(pca.result\$sdev\^2)/pca.result\$var.tot * 100}).


\paragraph{Other numerical outputs.}
PCA is a matrix decomposition of the original data matrix $X$ into a set of components of length $n$ and associated \textbf{loading vectors} of length $p$. A component is a linear combination of the original $p$ variables. To calculate components, we use the variable coefficient weights indicated in the loading vectors. Therefore, the absolute value of the coefficients in the loading vectors inform us on the importance of each variable to contribute to the definition of each component. We can extract this information through the \R{selectVar()} function which ranks the most important variables in decreasing order for each PC and outputs their weights (\texttt{value}) in the associated loading vector.


\begin{Schunk}
\begin{Sinput}
> # on the first component
> head(selectVar(pca.result, comp = 1)$name)
\end{Sinput}
\begin{Soutput}
[1] "A_42_P567268" "A_42_P493162" "A_43_P15711"  "A_43_P11754" 
[5] "A_43_P14324"  "A_42_P496622"
\end{Soutput}
\begin{Sinput}
> head(selectVar(pca.result, comp = 1)$value)
\end{Sinput}
\begin{Soutput}
               value.var
A_42_P567268  0.13356427
A_42_P493162  0.12585209
A_43_P15711   0.11553498
A_43_P11754   0.10697546
A_43_P14324   0.10053332
A_42_P496622 -0.09992275
\end{Soutput}
\end{Schunk}

A simple plot to display the absolute coefficient weights for the top 100 genes:

\begin{Schunk}
\begin{Sinput}
> plot(abs(selectVar(pca.result, comp = 1)$value)[1:100,1], type = 'h', 
+      ylab = 'Absolute loading weight')
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-012}

\paragraph{Some graphical outputs. }
We can plot one component against the other to visualise the samples projected into a reduced space spanned by the first PCs. Here we color each sample according to their treatment group:

\begin{Schunk}
\begin{Sinput}
> plotIndiv(pca.result, ind.names = FALSE, pch = 16, 
+           group = liver.toxicity$treatment$Dose.Group, 
+           legend = TRUE, title = 'PCA sample plot')
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-013}

We can also add the time of exposure to the treatment:

\begin{Schunk}
\begin{Sinput}
> plotIndiv(pca.result, ind.names = FALSE, group = liver.toxicity$treatment$Dose.Group, 
+           pch = as.factor(liver.toxicity$treatment$Time.Group), legend = TRUE, 
+           title = 'PCA sample plot', 
+           legend.title = 'Dose', legend.title.pch = 'Exposure')
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-014}

The correlation circle plots indicate the contribution of each variable to each component using the \R{plotVar()} function, as well as how correlated the variables are (indicated by a 'cluster' of variables). The plot might be crowded with the 3116 genes, so best is to set up a correlation coefficient cutoff. We can also indicate other gene ID names if those are available:

\begin{Schunk}
\begin{Sinput}
> plotVar(pca.result, cutoff = 0.8, cex = c(rel(1.8)),
+         var.names = list(liver.toxicity$gene.ID[,'geneBank']))
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-015}

More details on how to interpret a correlation circle plot is described in \citep{Gon12}.


\subsection{sparse PCA}\label{usage:sPCA}
\paragraph{Choosing the number of variables to select. }

In sPCA, the number of variables to select on each PC is input with the argument  \argu{keepX}. Tuning the  \argu{keepX} argument is not a straightforward task as the amount of explained variance increases with the number of variables that enter the model, even though those variables may contribute to noise!

Since sPCA is an unsupervised and exploratory technique, it is best to choose arbitrary  \argu{keepX} values that can be useful for preliminary interpretation, along with sample plot. 

The following example shows an arbitrary  \argu{keepX} to select the top (4, 6, 1) genes that contribute the most to the variance in the data on PCs 1, 2 and 3 respectively, by using the plots from the absolute coefficient weights from the PCA results above as a rough guide.

\begin{Schunk}
\begin{Sinput}
> spca.result <- spca(X, ncomp = 3, center = TRUE, scale = FALSE, 
+                     keepX = c(4, 6, 1))
> #spca.result
\end{Sinput}
\end{Schunk}

\paragraph{Variable selection. }
The \R{selectVar()} function  highlights the variables selected on each component. Compared to the head of the results from the PCA results above, we can observe that the top ranked variables on the first component are similar, but the following components start to differ as the sPCA's selection is based on the variables that were selected on the previous components.

\begin{Schunk}
\begin{Sinput}
> # First component
> selectVar(spca.result, comp = 1)$name
\end{Sinput}
\begin{Soutput}
[1] "A_42_P567268" "A_42_P493162" "A_43_P15711"  "A_43_P13297" 
\end{Soutput}
\begin{Sinput}
> # to compare with the top 4 genes in PCA:
> selectVar(pca.result, comp = 1)$name[1:4]
\end{Sinput}
\begin{Soutput}
[1] "A_42_P567268" "A_42_P493162" "A_43_P15711"  "A_43_P11754" 
\end{Soutput}
\begin{Sinput}
> # Second component
> selectVar(spca.result, comp = 2)$name
\end{Sinput}
\begin{Soutput}
[1] "A_43_P11710"  "A_43_P12028"  "A_42_P661746" "A_42_P594613"
[5] "A_43_P11230"  "A_43_P11468" 
\end{Soutput}
\begin{Sinput}
> # to compare with the top 6 genes in PCA:
> selectVar(pca.result, comp = 2)$name[1:6]
\end{Sinput}
\begin{Soutput}
[1] "A_43_P11681"  "A_42_P661746" "A_42_P493162" "A_43_P12028" 
[5] "A_42_P567268" "A_43_P15711" 
\end{Soutput}
\end{Schunk}

\paragraph{Some graphical outputs. }
The sample plot based on this very small selection of genes seem to show clear patterns on low doses (50 or 150 mg/kg) vs high doses (1500 or 2000 mg/kg) with short vs long exposure to acetaminophen:

\begin{Schunk}
\begin{Sinput}
> plotIndiv(spca.result, ind.names = FALSE, 
+           group = liver.toxicity$treatment$Dose.Group, 
+           pch = as.factor(liver.toxicity$treatment$Time.Group),
+           legend = TRUE, title = 'sPCA sample plot', 
+           legend.title = 'Dose', legend.title.pch = 'Exposure')
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-018}

Finally the correlation circle plot highlights the correlation structure between the selected genes. Note that here no cutoff threshold is needed if you selected a small number of variables:

\begin{Schunk}
\begin{Sinput}
> plotVar(spca.result, 
+         var.names = list(liver.toxicity$gene.ID[,'geneBank']))
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-019}


\subsection{Missing values}\label{usage:NIPALS}

All methodologies implemented in \texttt{mixOmics} can handle missing values. We use the non-linear iterative partial least squares algorithm NIPALS approach \citep{Wol66} to estimate missing values. NIPALS is based on the PLS algorithm which performs local regressions on the latent components to deal with missing values.

Missing values in \texttt{R} are usually indicated by the reserved word 'NA', which stands for 'Not Available'. In our context, NA values are missing by random, meaning that not a single row or single column contains missing values (those should be discarded beforehand). These values are NA usually because they were not detected by the technological platform (e.g. below a detection threshold) and no other values can replace themm i.e. they cannot be replaced by a 0 value. 

\paragraph{Example of missing data estimation.}
Here we create a reduced size data set from liver toxicity with only a 100 transcripts for illustrative purposes:

\begin{Schunk}
\begin{Sinput}
> X.subset <- liver.toxicity$gene[, 1:100] # 
> # create 20 NA values in our data in rows and columns
> na.row <- sample(1:nrow(X.subset), 20, replace = TRUE)
> na.col <- sample(1:ncol(X.subset), 20, replace = TRUE)
> # create a new data matrix with NA values
> X.na <- as.matrix(X.subset)
> # fill these NA values in X
> X.na[cbind(na.row, na.col)] <- NA
> sum(is.na(X.na)) # displays the number of NA values
\end{Sinput}
\begin{Soutput}
[1] 20
\end{Soutput}
\end{Schunk}

We then run the NIPALS algorithm, which is a PCA variant for missing values. Contrary to PCA, here we want to include as many components are possible (i.e. min($n$, $p$)) to estimate as best as possible the missing values. 

\begin{Schunk}
\begin{Sinput}
> # this might take some time depending on the size of the data set
> set.seed(33) # for reproducibility here
> # Nipals with a rather large number of components:
> nipals.X = nipals(X.na, reconst = TRUE, ncomp = 30)$rec
\end{Sinput}
\end{Schunk}

Sometimes a warning may appear for the calculation of a particular component if the algorithm struggles to converge when values are not missing by random.

We then only replace the missing values:

\begin{Schunk}
\begin{Sinput}
> #  only replace the imputation for the missing values
> id.na = is.na(X.na)
> nipals.X[!id.na] = X.subset[!id.na]
\end{Sinput}
\end{Schunk}

The quality of the estimation will depend on the complexity of the data:

\begin{Schunk}
\begin{Sinput}
> plot(
+     nipals.X[id.na],  # imputed values
+     X.subset[id.na],  # original non missing values
+     xlab = 'Imputed values', ylab = 'original values')
> abline(a = 0, b = 1) # how far are we from a perfect estimation on the diagonal line?
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-023}

\paragraph{When do I need to estimate missing values?}
In general we recommend to use this method only if 
\begin{enumerate} \itemsep0em 
  \item the method used to analyse the data cannot deal with missing values and 
  \item there are less than 20\% of the total dataset that are missing.
\end{enumerate}

Usually, if we observe a PCA barplot of the amount explained values that does not decrease, it is time to estimate missing values to carry on the analyses!

In our example, we could have carried on without NIPALS as the barplot did not show any warning sign:

\begin{Schunk}
\begin{Sinput}
> pca.result.NA <- pca(X.subset, ncomp = 5)
> plot(pca.result.NA)
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Methods/PCA/fig-024}


% --------------SECTION: maths background ----------------
\section*{Mathematical background}
Mathematical background for PCA and sPCA is given in Appendix \ref{math:PCA}.


\begin{Schunk}
\begin{Soutput}
Writing to file meth-PCA.R 
\end{Soutput}
\end{Schunk}
% 
% 
% \chapter{Case study with PCA on Multidrug data set}\label{study:MultidrugPCA}
% \SweaveInput{case-study-Multidrug.Rnw}
% 
%PLS-DA
\chapter{Discriminant Analysis}\label{method:PLSDA}
% !Rnw root = mixOmics-material-oneday.Rnw


% ---Redefine Sweave Style -----------------

\setkeys{Gin}{width=0.6\textwidth} 

 %,height=4}


\section{Why use PLS-DA?}
The Projection to Latent Structures (PLS) model was principally designed for regression problems, where the outcome, or response is continuous. However PLS is also known to perform very well for classification problems where the outcome is categorical \citep{Tan04}. We have implemented a Discriminant Analysis version of the PLS with good predictive performance and the identification, or selection, of discriminative variables. Contrary to Linear Discriminantn Analysis, the PLS-DA classifier can handle large data sets. In addition, PLS-DA performs `simultaneous' multiclass problems without having to decompose the problem into several binary subproblems.\\

In this supervised classification setting, the response variable $Y$ is a class vector (a \textbf{factor}) indicating the class of each sample. $X$ is a matrix of predictors (e.g. gene expression data set) and the aim is to predict the class of new test samples based on a trained classification model.

\medskip
\begin{mdframed}%[backgroundcolor=my-light-gray]
\begin{itemize}
  \item {\color{blue}PLS-Discriminant Analysis (PLS-DA)} is a linear model which performs a classification task and is able to predict the class of new samples \citep{Bar03}. 
  \item The sparse variant {\color{blue}sparse PLS-DA (sPLS-DA)} enables the selection of the most predictive or discriminative features in the data to classify the samples \citep{Lec11}.
\end{itemize}
\end{mdframed}
\medskip



\begin{figure}[!h]
\begin{center}
\includegraphics[angle=0,width=300pt]{Figures/Methods/PLSDA/PLS-DA-framework2} 
\end{center}
\caption{Schematic view of the PLSDA matrix decompositions into sets of PLS-DA component (latent variables) and loading vectors. $Y^{*}$ is the outcome factor coded as a dummy matrix, see Table \ref{Tab_yY}}\label{framework:PLSDA}
\end{figure}

% -------------- SECTION: Principle ------------------------
\section{Principle}


  \subsection{Partial Least Squares Discriminant-Analysis}
Although Partial Least Squares was not originally designed for classification and discrimination problems, it has often been used for that purpose \citep{Ngu02a, Tan04}. The response matrix $Y$ is qualitative and is internally recoded as a dummy block matrix that records the membership of each observation, i.e. each of the response categories are coded via an indicator variable (see Table \ref{Tab_yY}). The PLS regression (now PLS-DA) is then run as if $Y$ was a continuous matrix. This PLS classification trick works well in practice, as demonstrated in many references \citep{Bar03, Ngu02a, Bou07, Chung10}. 



\begin{table}[!h]
\caption{Example of an input outcome factor $Y$ (left) transformed into an indicator dummy matrix $Y^{*}$ (right). The transformation is performed internally in the  supervised multivariate methods.} \label{Tab_yY}
  \centering
\begin{tabular}{rc}
  \hline
 & Y \\ 
  \hline
indiv1 & trt1 \\ 
  indiv2 & trt1 \\ 
  indiv3 & trt1 \\ 
  indiv4 & trt1 \\ 
  indiv5 & trt2 \\ 
  indiv6 & trt2 \\ 
  indiv7 & trt2 \\ 
  indiv8 & trt2 \\ 
  indiv9 & trt3 \\ 
  indiv10 & trt3 \\ 
  indiv11 & trt3 \\ 
  indiv12 & trt3 \\ 
   \hline
\end{tabular}
\quad
\vrule \vrule
\quad
\begin{tabular}{rrrr}
  \hline
 & trt1 & trt2 & trt3 \\ 
  \hline
indiv1 & 1 & 0 & 0 \\ 
  indiv2 & 1 & 0 & 0 \\ 
  indiv3 & 1 & 0 & 0 \\ 
  indiv4 & 1 & 0 & 0 \\ 
  indiv5 & 0 & 1 & 0 \\ 
  indiv6 & 0 & 1 & 0 \\ 
  indiv7 & 0 & 1 & 0 \\ 
  indiv8 & 0 & 1 & 0 \\ 
  indiv9 & 0 & 0 & 1 \\ 
  indiv10 & 0 & 0 & 1 \\ 
  indiv11 & 0 & 0 & 1 \\ 
  indiv12 & 0 & 0 & 1 \\ 
   \hline
\end{tabular}
\end{table}


We use the following data input matrices: $X$ is a $ n \times p$ data matrix, $Y$ is a factor vector of length $n$ that indicates the class of each sample, and $Y^{*}$ is the associated dummy matrix  $ n \times K$ data matrix (see Table \ref{Tab_yY}), with $n$ the number of samples (individuals), $p$ the number of variables and $K$ the number of classes. %We will denote by $X^j$ the vector variables in the $X$ and the $Y$ datasets ($j = 1,\ldots ,p$ and $k = 1,\ldots ,q$). \\  
\textbf{A PLS-DA method outputs the following} (Figure \ref{framework:PLSDA}):
\begin{itemize}
\itemsep0em 
  \item A \textbf{set of components}, also called latent variates. There are as many components as the chosen \textit{dimension} of the PLS-DA model. 
  \item A  \textbf{set of loading vectors}, which are coefficients corresponding to each variable. Those coefficients indicate the importance of each variable in PLS-DA, they are used to calculate the components. Each component has its associated loading vector. Loading vectors are obtained so as to maximise the covariance ($\sim$ correlation) between a linear combination of the variables from $X$ (the $X$-component) and the factor of interest $Y$ (the $Y^{*}$-component.
\end{itemize}


  \subsection{sparse Partial Least Squares Discriminant-Analysis}
sPLS-DA performs variable selection and classification in a one step procedure. sPLS-DA is a special case of sparse PLS (not covered in this workshop), where the $l_1$ penalization applies only on the loading vector $a^h$ associated to the $X$ data set (see Fig. \ref{framework:PLSDA}).

  
% ------------------SECTION: Parameter tuning -----------------------
\section{Choosing the optimal parameters}\label{tuning:PLSDA}


  \subsection{Parameters during learning}
For PLS-DA, we need to specify the number of dimensions or components \argu{ncomp}. In addition, for sPLS-DA we need to specify the number of variables to select on each dimension \argu{keepX}. 

\paragraph{Misclassification error rate.} In this supervised framework, we use the estimation of the misclassification error rate using leave-one-out or cross-validation to guide the choice of these parameters. This can be achieved via the \R{perf()} and \R{tune()} functions, which internally call the \R{predict()} function. Both overall misclassification error rate and Balanced Error Rate (\textbf{BER}) are reported. BER is appropriate in case of an unbalanced number of samples per class as it calculates the average proportion of wrongly classified samples in each class, weighted by the number of samples in each class. Therefore, BER is less biased towards majority classes during the performance assessment. The choice of the parameters (described below) is made according to the best prediction accuracy, i.e. the lowest overall error rate or lowest BER.  

\paragraph{Cross-validation.} 
Cross-validation (CV) is a model validation technique used in statistical and machine learning to assess whether the results of an analysis can be generalised to an independent data set. It consists in dividing the data set into $s$ subsets (or folds), fitting the model on $s-1$ subsets and evaluating the prediction performance on the left-out subset. This process is iterated until each subset is left out once; the prediction performance are then averaged. In our methods, prediction performance refers to either the overall misclassification error rate or the BER calculated on the left-out samples. \\
We define \textit{stratified CV} when there is approximately the same proportion of each class in each of the folds. Repeated cross-validation implies that the whole CV process is repeated a number of times  \argu{nrepeat} to reduce variability across the different subset partitions. In the case of Leave-One-Out CV (\argu{validation = `loo'} in \R{tune()} and \R{perf()}), each sample is left out once ($s = N$) and therefore \texttt{nrepeat} is set to $1$. \\
In \R{perf()}, besides the classification performance criterion, the choice of \argu{keepX} can also be complemented with a stability criterion to assess how often variable are selected across different cross-validation folds. \\ %More details about the functions can be found in Section \ref{param:PLSDA} below. \\
In \citet{Lec11}, we showed that for most cases, the user could set \R{ncomp = }$K-1$, with $K$ the number of classes. This is similar to what is advised for the classical Linear Discriminant Analysis case and works well in practice. In practice we use the \R{perf()} function on a $K+1$ number of components on a PLS-DA model to choose \argu{ncomp}. We then use \R{tune()} on sPLS-DA on a grid of \argu{keepX} values to choose \argu{keepX}. \\
Note that tuning the number of variables to select cannot be accurately estimated is the number of samples is small. In that case it is better to set \argu{keepX} to an arbitrary choice.

  \subsection{Prediction distances}\label{param:PLSDA-dist}
Different prediction methods are proposed and implemented in the functions \R{predict()},  \R{tune()} and \R{perf()} to assign to each predicted score of a test sample a final predicted class. Those distances are:
\begin{itemize}
\itemsep0em 
  \item \argu{method.predict = "max.dist"}
  \item  \argu{method.predict = "centroids.dist"} 
  \item \argu{method.predict = "mahalanobis.dist"} 
\end{itemize}
The difference between the distances are described in Appendix \ref{math:PLSDA}. We found that the centroid-based distances, and specifically the Mahalanobis distance led to more accurate predictions than the maximum distance for complex classification problems and N-integration problems. 
The tuning step will indicate which distance leads to the best performance of the model (outputs from the functions \R{tune()} and \R{perf()}), as it is data specific.  

\subsection{Avoid overfitting in sPLS-DA with the \texttt{mixOmics} tuning framework}
The issue of selection bias and overfitting in variable selection problems needs to be carefully considered with sPLS-DA \citep{Amb02}. Indeed, given the large number of variables, sPLS-DA is almost always able to weight the appropriate variables able to discriminate the classes. However, the method (classifier)  based on this resulting variable selection may not be able to generalise to a new data set, or to the same learning data set where some samples were removed. In other words, the sPLS-DA classifier and its variable selection `sticks' too much to the learning set. This is why we use cross-validation during the tuning process. \\
  
We propose the following framework in \R{mixOmics} which is illustrated in our case studies with PLS-DA to avoid overfitting.
%  \begin{algo} \textbf{Framework to tune the sPLS-DA parameters in \texttt{mixOmmics}}.
  \begin{enumerate} \itemsep0em 
    \item \textbf{Choice of number of components:} Tune \R{ncomp} on a sufficient number of components using a PLS-DA model, with the \R{perf()} function. Set the \argu{nrepeat} argument that will repeat cross-validation for an accurate performance estimation. Choose the optimal \R{ncomp = H} along with its prediction distance that yields, on average, the lowest misclassification error rate based on both mean \textit{and} standard deviation. 
    \item \textbf{Choice of number of variables to select:} Now that \R{ncomp} is chosen, run sPLS-DA on a grid of \R{keepX} values (the larger and finer the grid the longer it will take to compute!) with the \R{tune()} function which \textit{internally} performs the following steps:
    \item \label{itm:tuning}For each component \R{h} \itemsep0em
  \begin{enumerate}  \itemsep0em 
    \item Performance assessment of sPLS-DA for each \R{keepX} value
    \item The \R{keepX} value with the lowest average error rate is retained and the performance is assessed similarly for the next component \R{h+1} .
    %\item \label{itm:stability}Save the variable stability measure for later use.
  \end{enumerate} 
    \item \label{itm:model}\textbf{Final model:} Run sPLS-DA with the \argu{ncomp, keepX} values that are output or chosen on the \textbf{whole} data set.
    \item \label{itm:assess} \textbf{Final model assessment:} Run the \R{perf()} function on the final sPLS-DA model with the argument \argu{nrepeat} to accurately assess the performance and examine the stability of the variables selected.
    \item \label{itm:sig}\textbf{Variable signature:} Retrieve the list of selected variables using \R{selectVar()}\!\!. You can then cross-compare with the stability of those variables. % in step \ref{itm:stability}
    \item \label{itm:pred}\textbf{Prediction:} If an external test set is available, predict the class of the external observations using the function \R{predict()} and assess how generalisable your results are!
  \end{enumerate}
%  \end{algo}
  
\paragraph{Can I skip the tuning step?}
  Sometimes the number of samples does not allow for cross-validation or leave-one-out cross-validation. In that case an exploratory approach can be adopted, where the \argu{ncomp, keepX} are chosen \textit{apriori}. Steps \ref{itm:model} and \ref{itm:sig} can still be performed, with some caution when intepreting the results.

%  Note that in the case of cross-validaton, the step \ref{itm:tuning} should be repeated several times and the error rate averaged across the repeats, see the Case study \ref{study:SRBCT}.
  
%\section{Current and future extensions in 6.3.0}
%The tuning the \argu{keepX} parameter is chosen using one-sided t-tests based on the misclassification error rate. The framework presented above is now implemented in a Shiny web-interface (beta version): \url{http://mixomics-projects.di.uq.edu.au/Shiny/}. Contact us if you have more questions about the current and future extensions.


%-- SECTION: Usage in mixOmics -----------------------
%-----------------------------------------------------
\section{Usage in mixOmics}\label{usage:PLSDA}
A full guided example is presented in \url{http://mixomics.org/case-studies/splsda-srbct/} %Chapter \ref{study:SRBCT}.

% --------------SECTION: maths background ----------------
\section*{Mathematical background}
Mathematical background for PLS-DA and sPLS-DA is available in Appendix \ref{math:PLSDA}.



% 
% \chapter{Case study with sPLS-DA on SRBCT data set}\label{study:SRBCT}
% \SweaveInput{case-study-SRBCT.Rnw}
% 
\chapter{Case study with PCA and sPLS-DA for 16S microbiome data}\label{study:mixMC}
% !Rnw root = mixOmics-material-oneday.Rnw

% ---Redefine Sweave Style -----------------

\setkeys{Gin}{width=0.8\textwidth} 

 %,height=4}



% --------------------------
%\section{\texttt{mixMC}: a multivariate framework for microbiome data analysis}
% --------------------------
\section{Microbiome data}
Culture independent techniques, such as shotgun metagenomics and 16S rRNA amplicon sequencing have dramatically changed the way we can examine microbial communities. However, current statistical methods are limited in their scope to identifying and comparing bacteria driving changes in their ecosystem. This is partly due to the inherent properties of microbiome data.
The absence of microbial organisms from a large number of samples results in highly skewed count data with many zeros (\textit{sparse} count data). In addition, the varying sampling/sequencing depth between samples requires transformation of the count data into relative abundance (proportions) leading to \textit{compositional} data. 

\subsection{Methods improvements for microbiome 16S data analysis} 
Compositional data pose statistical theoretical issues and potentially considerable misinterpretation with standard methods \citep{Ait82} as such data within a specimen sample sum to one, resulting in data residing in a simplex, rather than an Euclidian space. The solution proposed by several authors is to project the relative count data into an Euclidian space using log ratio transformations, such as centred log ratio transformation (CLR), before applying standard statistical techniques. The CLR transformation consists in dividing each sample by the geometric mean of its values and taking the logarithm \citep{Ait82, Fer14}. The transformation is symmetric, resulting in the retention of dimensions in the data \citep{Fil09}. %CLR definition in supp
%ILR is another transformation that has been proposed by  \citep{Fil09} for Principal Component Analysis but leads to projected data of size . As PCA is an unsupervised method

Our multivariate framework \R{mixMC} takes into account sparsity and compositionality of the data, and aims to identify specific associations between microbial communities and their type of habitat \citep{Lec16}. %Our hypothesis is that multivariate methods can help identify microbial communities that modulate and influence biological systems as a whole. 
The method sPLS-DA was improved with CLR transformation and includes a multilevel decomposition for repeated measurements design that are commonly encountered in microbiome studies. The multilevel approach from \cite{Wes10} enables the detection of subtle differences when high inter-subject variability is present due to microbial sampling performed repeatedly on the same subjects but in multiple habitats. To account for subject variability the data variance is decomposed into  `\textit{within variation}' (due to habitant) and `\textit{between subject variation}' \citet{Liq12}, similar to a within-subjects ANOVA in univariate analyses. 
%. In univariate analyses, this step refers to repeated measures ANOVA (also called within-subjects ANOVA). In multivariate analysis we refer to a multilevel approach as proposed by \cite{Wes10} and further expanded in \R{mixOmics} in \citet{Liq12}. 

Graphical outputs such as \R{plotLoadings()} can be used to represent the habitat in which the selected micro-organism is most present.

\subsection{Parameters tuning}
The choice of the parameters is similar to the sPLS-DA described in Sections \ref{tuning:PLSDA}. Briefly, the data are first CLR transformed and, if applicable, multilevel decomposed on each cross-validated training set to avoid bias the performance evaluation in the \R{tune()} function. 

%Through data dimension reduction the multivariate methods provide insightful graphical visualizations to characterize each type of environment in a detailed manner. We will illustrate the added value of using multivariate methodologies to fully characterize and compare microbial communities.

\subsection{Data input} For 16S data, we assume that raw counts first offset to avoid 0 values (data = raw count + 1), were prefiltered to remove OTUs with low counts, and their relative abundance was calculated using Total Sum Scaling (TSS, see details in \url{http://mixomics.org/mixmc/prefiltering/} and \url{http://mixomics.org/mixmc/normalisation/}). In the example below we provide the TSS data. Raw +1 data as well as Cumulative Sum Scaling data are also provided (for the latter, no log ratio transformation is necessary).

Examples of analyses are presented in \url{http://mixomics.org/mixMC/} and in \citep{Lec16}.


% --------------------------
\section{Application to the Human Microbiome Project}
% --------------------------

We illustrate mixMC on 16S microbiome data measured on 162 samples corresponding to 54 unique healthy individuals. The outcome of interest $Y$ is the bodysite (Antecubital fossa, Stool or Subgingival plaque). After pre-filtering (see \url{http://mixomics.org/mixmc/prefiltering/}), a total of 1,674 OTUs were retained for analysis. The data were normalised using Total Sum Scaling (see details in \citealt{Lec16}).

Since the microbiome is samples on the same individuals but at different bodysites, we will use a multilevel analysis.


\begin{Schunk}
\begin{Sinput}
> library(mixOmics)
> data("diverse.16S")
> # the 16S normalised data
> data.mixMC = diverse.16S$data.TSS
> #dim(data.mixMC)
> 
> # the outcome 
> Y = diverse.16S$bodysite
> #summary(Y)
> 
> # unique ID of each individual for multilevel analysis
> sample = diverse.16S$sample
\end{Sinput}
\end{Schunk}

We first start with a PCA sample plot, specifying the log ratio transformation and the multilevel analysis.
\setkeys{Gin}{width=0.4\textwidth} 
\begin{center}
\begin{Schunk}
\begin{Sinput}
> pca.res = pca(data.mixMC, ncomp = 10, logratio = 'CLR', multilevel = sample)
> #pca.res
> plot(pca.res)
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-027}
\end{center}
\setkeys{Gin}{width=0.5\textwidth} 

The PCA sample plot shows a good clustering of the body sites. 

\begin{center}
\begin{Schunk}
\begin{Sinput}
> plotIndiv(pca.res, comp = c(1,2), # the components to plot
+           pch = 16, ind.names = F, group = Y, col.per.group = color.mixo(1:3),
+           legend = TRUE)
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-pca-plotIndiv}
\end{center}

The sPLS-DA tuning step is performed on 3 components, using 5-fold validation repeated 10 times.



\begin{Schunk}
\begin{Sinput}
> # this chunk takes ~ 5 min to run, participants can choose not to run 
> # that part and load the .RData provided instead
> # load('RData/mixMC-tune-sPLSDA.RData')
> set.seed(30)  # for reproducibility issues with participants
> splsda.tune = tune.splsda(data.mixMC, Y, ncomp = 3, multilevel = sample,
+                           logratio = 'CLR',
+                           test.keepX = c(seq(5,150, 5)), validation = 'Mfold', folds = 5, 
+                           dist = 'max.dist', nrepeat = 50)
> splsda.tune$choice.keepX
> splsda.tune$error.rate
\end{Sinput}
\end{Schunk}



The plot below shows the average error rate with respect to the \argu{keepX} values tested:
\setkeys{Gin}{width=0.7\textwidth} 
\begin{center}
\begin{Schunk}
\begin{Sinput}
> plot(splsda.tune)
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-032}
\end{center}
\setkeys{Gin}{width=0.5\textwidth} 

The error rate shows that 3 components are sufficient to achieve the best separation. We thus select the first three optimal keepX values: 5, 85, 5 in the final sPLS-DA model.

\begin{center}
\begin{Schunk}
\begin{Sinput}
> splsda.res = splsda(X = data.mixMC, Y = Y, multilevel = sample, logratio='CLR',
+                     ncomp = 3, keepX = splsda.tune$choice.keepX[1:3])
> plotIndiv(splsda.res, ind.names = F, col.per.group = color.mixo(1:3), comp = c(1,2), 
+           pch = 16, ellipse = TRUE, legend = TRUE)
> 
> # rerun plotIndiv for component 1 and 3.
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-033}
\end{center}

The code below outputs the first selected OTUs and their coefficient (from the loading vector) on the first component:
\begin{Schunk}
\begin{Sinput}
> head(selectVar(splsda.res, comp = 1)$value)  # just a head
\end{Sinput}
\begin{Soutput}
              value.var
OTU_97.38174 -0.6421273
OTU_97.39439 -0.5842682
OTU_97.108   -0.3633238
OTU_97.20    -0.2927394
OTU_97.39456 -0.1691232
\end{Soutput}
\end{Schunk}

The contribution plots are displayed with the Family taxonomy information on each selected OTU, as specified using the argument \argu{name.var}. Other arguments to improve the graphic include \argu{size.name} and \argu{ndisplay}, see \argu{?plotLoadings}. \textit{Have a look at component 3 and conclude on the benefit or relevance of adding that component in the model.}

\setkeys{Gin}{width=0.7\textwidth} 
\begin{center}
\begin{Schunk}
\begin{Sinput}
> par(mfrow=c(1,3))
> plotLoadings(splsda.res,  contrib = 'max', method = 'median', comp = 1, 
+              name.var = diverse.16S$taxonomy[, 'Family'], size.name = 1, 
+              legend = FALSE)
> plotLoadings(splsda.res,  contrib = 'max', method = 'median', comp = 2, 
+              name.var = diverse.16S$taxonomy[, 'Family'], size.name = 0.3, 
+              legend.title = 'Body site')
> par(mfrow=c(1,1))
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-035}
\end{center}
\setkeys{Gin}{width=0.55\textwidth} 

The CIM is displayed below and shows a separation of the bodysites. The CIM include all OTUs selected on all 3 components from the sPLS-DA model. \textit{Rerun a part of the analysis to only show the OTUs selected on 2 components. Is the separation of the bodysites improved in CIM?}

\begin{center}
\begin{Schunk}
\begin{Sinput}
> cim(splsda.res, row.sideColors = color.mixo(Y))
> # for display issues with Rstudio, use the arguments 
> # save = 'pdf' and name.save = 'myCIM' instead
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-036}
\end{center}

The classification performance of the sPLS-DA multilevel model can be assessed using the function \R{perf()}. The mean error rates per component are output and type of distance are output.

\setkeys{Gin}{width=0.6\textwidth} 
\begin{center}
\begin{Schunk}
\begin{Sinput}
> splsda.perf = perf(splsda.res, validation = 'Mfold', folds = 5, 
+                    progressBar = FALSE, nrepeat = 10)
> splsda.perf$error.rate
\end{Sinput}
\begin{Soutput}
$overall
         max.dist centroids.dist mahalanobis.dist
comp 1 0.33333333     0.20493827       0.20493827
comp 2 0.01604938     0.08580247       0.03271605
comp 3 0.03703704     0.08950617       0.04567901

$BER
         max.dist centroids.dist mahalanobis.dist
comp 1 0.33333333     0.20493827       0.20493827
comp 2 0.01604938     0.08580247       0.03271605
comp 3 0.03703704     0.08950617       0.04567901
\end{Soutput}
\begin{Sinput}
> plot(splsda.perf)
\end{Sinput}
\end{Schunk}
\includegraphics{Figures/Studies/mixMC/fig-037}
\end{center}


% --------------------------
\section{To go further}
% --------------------------
We provide two other examples:
\begin{itemize}
\item Another 16S data is available, see \R{?Koren.16S}\! where you can test your newly acquired \R{mixOmics} analytical skills for a full data exploration and sPLS-DA analysis (non multilevel) with 16S data (analysed in \citet{Lec16} and microbial signatures compared with those obtained with the HMP data).
\item An integrative method based on kernel methods from our collaborators Drs Jerome Mariette and Nathalie Villa (French National Institute for Agricultural Research) is also featured here: \url{http://mixomics.org/mixkernel/}
\end{itemize}


\begin{Schunk}
\begin{Sinput}
> data("Koren.16S")
> #?Koren.16S
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Soutput}
Writing to file case-study-microbiome.R 
\end{Soutput}
\end{Schunk}



% \chapter{Graphical outputs for integrative analyses}\label{graph:outputs}
% \SweaveInput{meth-graphics.Rnw}
% 
% \chapter{$N-$ integration and example}\label{study:multiblock}
% \SweaveInput{case-study-multiblock.Rnw}


%% Xtra not shown
% % \chapter{Multivariate analyses on Arabidopsis data, your turn!}\label{study:arabi}
% % \SweaveInput{study-arabidopsis-short.Rnw}

% % % % -----------------------------------------------
% % % %       Appendix
% % % % ----------------------------------------------
\appendix
% !Rnw root = mixOmics-material-short.Rnw

\chapter{Mathematical background}
% ===========================
% section: PCA maths
% ===========================

\section{PCA}\label{math:PCA}

The content of this section addresses mathematical aspects of PCA, sPCA and NIPALS which will be omitted during the workshop. \\

\noindent
In the package, PCA is numerically solved in two ways:
\begin{itemize}
  \item with singular value decomposition (SVD) of the data matrix, which is the most computationally efficient way and is also adopted by most softwares and the R function \R{prcomp()} in the stat package, 
  \item with the Non-linear Iterative Partial Least Squares (NIPALS) in the case of missing values, which uses an iterative power method.
\end{itemize}
Both methods are embedded in the \R{mixOmics pca()} function and will be used accordingly.

\subsection{Notations} 
Suppose $X$ is a $ n \times p$ data matrix of rank $r$ (\mbox{rank}($X) = r$), with $n$ the number of samples (patients) and $p$ the number of variables.
We will denote by $X_{h}$ the (current) residual matrices ($h = 1, \ldots ,r$) and by $x_{ij}^h$ the measurement of the variable $j$ in individual $i$ in the current matrix $X_h$ ($i = 1, \ldots ,n$; $j = 1, \ldots ,p$). %For example $a_j^h$ is the weight of the variable $j$ in the loading vector $a^{h}$.

\subsection{Objective function}
The objective function to solve is
\begin{equation}\label{obj:PCA}
\mbox{arg}\max_{||a^{h}|| = 1} \mbox{var}(Xa^h)
\end{equation}

\noindent where $a^{h}$ is the  $p-$ dimensional loading vector associated to the principal component $h$, $h = 1, \dots ,r$, under the constraints that $a^{h}$ is of unit (norm) 1 and is orthogonal to the previous loading vectors $a^m$, $m < h$. The principal component vector is defined as $t^h = Xa^h$.


\subsection{PCA with Singular Value Decomposition}
Equation \ref{obj:PCA} can be solved by computing the eigenvectors $a^1, \dots, a^r$ and associated eigenvalues $\delta^2, \dots, \delta^2$ of the variance covariance matrix $X^TX$ \footnote{In this material we only consider the case of the Identity metric to measure the distance between samples. This is generally the default case in statistical softwares}. The eigenvalues are ordered $\delta^1 \leq \delta^2 \leq \dots \leq \delta^r$ and are proportional to the amount of explained variance on each dimension (component).\\

\noindent
A more efficient way is to use the Singular Value Decomposition (SVD) of the data matrix $X$:
$$
X = U\Delta A^T
$$
where the columns of $U$ are orthogonal and of norm 1 and the columns of $A$ are the loading vectors. The matrix $\Delta = \mbox{diag}(\delta_1, \dots, \delta_r)$ contains the singular values (the square roots of the eigenvalues of $X^TX$). The principal components are the columns of $T = U \Delta$. \\

Computing the SVD of $X$ is more efficient that calculating the matrix product $X^TX$ which is of size $p \times p$. The SVD is nowadays the standard way to calculate PCA.


\subsection{PCA with Non-linear Iterative Partial Least Squares algorithm}\label{math:NIPALS}
We have seen that PCA seeks for the vectors $a^h$ that maximize the variance of the associated principal components and that the principal components are defined as $t^h = Xa^h$ ($h = 1, \dots, r$). \\

The iterative way of solving PCA can be summarised in this algorithm \citep{Ten98}:
\medskip

\begin{algo}\label{algo:NIPALS} NIPALS with no missing values.
\begin{itemize}
  \item Initialize $X_0 = X$
  \item For $h$ in $1,\ldots ,r$:
  \begin{enumerate}
    \item \label{itm:nipals.init} Initialize $t^1 = \delta_1 a^1$ using the first left singular vector and singular value of SVD($X$) %\footnote{Originally, this initialization step proposed}    
    %X_h[,1]$ %the component $t^h$ to the first column of the current data matrix:
    \item \label{itm:nipals.conv} Until convergence of $a^h$:
      \begin{enumerate}
        \item \label{itm:nipals.reg1} $a^h = X_{h-1}^T t^h / t^{h'}t^h$
        \item Norm $a^h$ to 1
        \item \label{itm:nipals.reg2} $t^h = X_{h-1} a^h$ % / \boldsymbol{v}_{h'}\boldsymbol{v}_h$
      \end{enumerate}
      \item \label{itm:nipals.defla} $X_{h} = X_{h-1} - t^ha^{h'}$
    \end{enumerate}
\end{itemize}
\end{algo}
\bigskip

Step \ref{itm:nipals.reg1} performs a \textbf{local regression} of each variable $j$ ($x_{h-1,j}$) onto the component $t^h$ to obtain the associated weight of each variable. We can therefore consider $a^{h}$ as the slope of the means squares regression line between the $n$ data points $(t_{h, i}, x_{h-1,ji})$, $j = 1, \dots ,n$, $i = 1, \dots ,p$. Similarly in step \ref{itm:nipals.reg2}, each element in $t^{h}$ ($t_{h,i}$) is the regression coefficient of $x_{h-1,ji}$ onto $a^{h}$. \\

Step \ref{itm:nipals.defla} is called the \textbf{deflation step}, where $X_{h}$ is the residual matrix of the regression of $X_{h-1}$ onto $t^h$. From this step we can see how the algorithm is iteratively decomposing the matrix $X$ with a set of vectors ( $t^h, a^{{h'}}$) for each dimension $h$.\\

Since NIPALS performs local linear regressions for each PCA dimension, it becomes easier to understand how the algorithm can deal with missing values. The absence of some values in the variables $j$ does not indeed impede (too much) on the local regressions (steps \ref{itm:nipals.reg1} and \ref{itm:nipals.reg2}), as these are fitted on the existing values. The missing values are reconstructed using the formula
\begin{equation}\label{eq:nipals}
\hat{x}_{ji} = \sum_{l=1}^h t_{li}a_{li},
\end{equation}
where $x_{ji}$ is the missing value for sample $i$ and variables $j$ and $\hat{x}_{ji}$ is the estimation of the missing value. We can see from Equation \ref{eq:nipals} that the estimation is made to the order $h$, where $h$ is the chosen number of components in NIPALS. This explains why NIPALS requires quite a large number of components in order to estimate these missing values. \\

Note that in the case of missing values, the component $t^h$ is initialised to the first column of the current data matrix in step \ref{itm:nipals.init}: $t^h = X_{h-1}[\;,1]$.   


\subsection{sparse PCA}\label{meth:sPCA}
Sparse PCA from \citet{She07} uses the low rank approximation property of the SVD and the close link between low rank approximation of matrices and least squares regression (as highlighted above with NIPALS).\\
\noindent
We define the Frobenius norm between $X$ and its rank-$l$ matrix $X^{(l)}$ as:
$$
 ||X - X^{(l)}||^2_F = \mbox{trace}\{ (X - X^{(l)})(X - X^{(l)})^T \}.
$$
And we define the closest rank-$l$ matrix approximation to $X$ in terms of the squared Frobenius norm as:
\begin{equation}\label{sPCA:rank}
  X^{(l)} \equiv \sum_{k=1}^l \delta_k u^k a^{k'}.
\end{equation}
From Equation \ref{sPCA:rank}, we can understand how to obtain the best rank-$1$ approximation of $X$ by seeking the $n$- and $p$- dimensional vectors $t$ and $a$ (both of norm 1)

$$
 \min_{t, a}||X - t a'||^2_F,
$$
which we can solve by using the first left and right singular vectors $(u^1, a^1)$ from the SVD: $t = \delta_1 u^1$ and $a =  a^1$. The second set of vectors will give the best approximation of the rank-2 of $X$ (or equivalently of $X - \delta_1 u^1 a^{1'}$). \\

Steps \ref{itm:nipals.reg1} and \ref{itm:nipals.reg2} in Section \ref{math:NIPALS} explained how least squares regressions are performed within the PCA framework to obtain $a$ as a regression of $X$ on a fixed $t$. In such a regression context, it is then possible to apply regularization penalties on $a$ to obtain a \textbf{sparse} loading vector and therefore perform variable selection. \\
\noindent
The objective function of sPCA can be written as
$$
 \min_{t, a}||X - t a'||^2_F + P_{pen}(a),
$$
where $P_{pen}(a)$ % = \sum_{i=1}^p P_{pen}(|{v}_i|)$ 
is a penalty function with the tuning parameter $pen$ that is applied on each element of the vector $a$. The sPCA version in \R{mixOmics} implements the soft thresholding ($L_1$ or Lasso, \citealt{Tib96}) penalty which is applied on each variable $i$ as follows:
$$
  P_{pen}({a}^i) = \mbox{sign}({a}^i)(|{a}^i| - pen)_+,
$$
with the notation $(x)_+ \Leftrightarrow x = 0 \quad \mbox{if} \quad  x \leq 0 \quad \mbox{and} \quad x = x \quad \mbox{otherwise}$. The penalisation results in a loading vector with many 0 values for the variables that are considered irrelevant for the local regression in step \ref{itm:nipals.reg1}. Since $t = Xa$, the consequence is that the principal component $t$ is now calculated on a subset of relevant variables with non-zero weights in the sparse loading vector. The selected variables can be extracted by looking at the non-zero elements in the loading vector (see the function \R{select.var()}). \\

In \R{mixOmics} the penalty $pen$ has been replaced by the number of variables to select \argu{keepX} for practical reasons, but both criteria are equivalent. See Section \ref{tuning:sPCA} for the tuning of that parameter.\\

The pseudo code below explains how to obtain a sparse loading vector associated to the first principal component.
\medskip

\begin{algo}\label{algo:sPCA} sparse PCA for the first dimension $\boldsymbol{h = 1}$.
  \begin{enumerate}
    \item Extract the first left and right singular vectors (of norm 1) of the SVD($X_h$) to initialize 
    
    $t^1 = \delta_1  u^1$ and $a^h = a^1$
    \item Until convergence of $a^h$:
      \begin{enumerate}
        \item $a^h = P_{pen}(X_h^T t^h)$
        \item $t^h =  X_h a^h$
        \item Norm $t^h$ to 1
      \end{enumerate}
      \item Norm $a^h$ to 1  
  \end{enumerate}
\end{algo} 
\bigskip

Since sPCA is an iterative procedure, we can then include algorithm \ref{algo:sPCA} into step \ref{itm:nipals.conv} of the NIPALS algorithm \ref{algo:NIPALS} to extend the sPCA to the following components. \\

The sparse loading vectors are computed componentwise which results in a list of selected variable per component. The deflation step `should' ensure that the loading vectors are orthogonal to each other (i.e. different variables selected on different dimensions), but that also depends on the degree of penalty applied.

% =======================
% Section: Cor / var-cov
% ======================
\section{Correlation and variance-covariance matrices}\label{appendix:cov}
  The covariance matrix (also known as dispersion matrix or variance - covariance matrix) is fundamental to several statistical analyses, including PLS and CCA methods. It generalizes the notion of variance to multiple variables. \\
  A close cousin to the covariance matrix is the \textbf{correlation matrix}, which is a table reporting every pairwise correlation between two numerical variables. \\
\paragraph{Example.}
  Pearson's correlation matrix on the Iris data with the 4 variables \texttt{Sepal.Length, Sepal.Width, Petal.Length, Petal.Width}:
\begin{Schunk}
\begin{Soutput}
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
\end{Soutput}
\end{Schunk}
We can see that there is a high correlation between Sepal and Petal Length ($\textbf{r} = 0.87$).\\

The correlation matrix is symmetric and all the correlations on the diagonal are equal to 1  as they are the correlation of each variable with itself. \\

A \textbf{covariance matrix} for a data matrix composed of 4 variables can be written as:
$$  
 \begin{pmatrix}
\mbox{Var}_{1} & \mbox{Cov}_{1,2} &  \mbox{Cov}_{1,3} &  \mbox{Cov}_{1,4} \\
  \mbox{Cov}_{2,1} &  \mbox{Var}_{2} &  \mbox{Cov}_{2,3} & \mbox{Cov}_{2,4} \\
  \mbox{Cov}_{3,1} &  \mbox{Cov}_{3,2} &  \mbox{Var}_{3}  & \mbox{Cov}_{3,4} \\
  \mbox{Cov}_{4,1} &  \mbox{Cov}_{4,2} & \mbox{Cov}_{4,3} \ & \mbox{Var}_{4} \\
 \end{pmatrix}
$$  
A covariance matrix is very similar to a correlation matrix, except for two differences:
\begin{enumerate}
  \item The covariance between two variables is an \textit{unstandardized} version of their correlation (remember that in the correlation coefficient we divide the covariance by the standard deviation of both variables to remove units of measurement).  The covariance is therefore a correlation measured in the units of the original variables.
  \item Contrary to the correlation coefficient, the covariance coefficient is not between -1 or 1. Similar to a correlation coefficient, a value of 0 indicates no linear relationship.
\end{enumerate}
Note that since the covariance is in the original units of the variables, variables on scales with bigger numbers, and with wider distributions, will necessarily have bigger covariances.
%Because covariance is in the original units of the variables, variables on scales with bigger numbers, and with wider distributions, will necessarily have bigger covariances. So for example, Life Span has similar correlations to Weight and Exposure while sleeping, both around .3.
Below is the variance-covariance matrix of the Iris data:
\begin{Schunk}
\begin{Soutput}
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707
Sepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394
Petal.Length    1.2743154  -0.3296564    3.1162779   1.2956094
Petal.Width     0.5162707  -0.1216394    1.2956094   0.5810063
\end{Soutput}
\end{Schunk}



% ===============================
% section: PLS-DA maths
% ===============================
\section{PLS-DA}\label{math:PLSDA}
The content of this section addresses mathematical aspects of PLS-DA and sPLS-DA which will be omitted during the workshop. 

\paragraph{Notations.} We denote $X$ is a $ n \times p$ data matrix, $Y$ is a factor vector of length $n$ that indicates the class of each sample, and $Y^{*}$ is the associated dummy matrix  $ n \times K$ data matrix (see Table \ref{Tab_yY}), with $n$ the number of samples (individuals), $p$ the number of variables and $K$ the number of classes. %We will denote by $X^j$ the vector variables in the $X$ and the $Y$ datasets ($j = 1,\ldots ,p$ and $k = 1,\ldots ,q$). \\  

The objective function to solve is the same as the classical PLS objective function and involves maximizing the covariance between each linear combination of the variables from both groups. 
  $$ \arg \max_{||a^h||=1,\, ||b^h||=1} \mbox{cov}(X a^h, Y^{*} b^h) \qquad h= 1, \ldots ,H.
	$$

The \textbf{loading vectors} are the vectors $a^h$ and $b^h$ for each PLS dimension $h$, and the associated \textbf{latent variables} are denoted $t^h = X a^h$  and $u^h = Y^{*}b^h$. The loading vectors $a^h$ and $b^h$ are the $h$th left and right singular vector of the singular value decomposition (SVD) of $X'Y^{*}$ respectively for each iteration or dimension $h$ of the PLS. \\

  \subsection{Prediction with PLS-DA}

For a classification context, where the aim is to predict the class of new samples in a trained model, the PLS model used has a regression mode that is set internally in the function in \texttt{mixOmics}. The PLSDA model is formulated as:
$$ Y^{*} = X \beta + E,
$$
where $\beta$ is the matrix of the regression coefficients and $E$ is the residual matrix, see more details in Section \ref{math:PLSDA}. The prediction of a new set of samples is then
$$ Y^{*}_{new} = X_{new} \beta,
$$
Several \textbf{distances} were implemented to identify/predict the class membership of each new sample (each row in $Y^{*}_{new}$), as described below. 

%. For example, we can assign the predicted class as the column index of the element with the largest predicted value in this row (\R{method.predict = max.dist} in the \R{predict()} function). We give more mathematical details of the prediction distances in \citealt{mixOmics} supplemental material. 

\paragraph{Prediction distances.}
\begin{itemize}
  \item \argu{method.predict = "max.dist"} is the simplest method to predict the class of a test sample.  For each new individual, the class with the largest predicted score is the predicted class. In practice we found that this distance worked well for multiclass problems \cite{Lec11}.
\end{itemize}


For the centroid-based distances, we first calculate the centroid $G_k$ of all the learning set samples belonging to the class $k \le K$ based on the $H$ latent components $t^h$ associated to $X$. Both `Mahalanobis distance' and `Centroids distance' distances are applied on the predicted scores. 
\begin{itemize}
\itemsep0em 
  \item  \argu{method.predict = "centroids.dist"} allocates the new sample the class that mimimises the Euclidean distance dist($t^h$, $G_k$). 

  \item \argu{method.predict = "mahalanobis.dist"} uses the Mahalanobis metric in the calculation of the distance.
\end{itemize}

In practice we found that the centroid-based distances, and specifically the Mahalanobis distance led to more accurate predictions than the maximum distance for complex classification problems and N-integration problems. The centroid distances consider the prediction in a $H$ dimensional space using the predicted scores, while the maximum distance considers a single point estimate using the predicted dummy variables on the last dimension of the model. More details are given in \citealt{Roh17} supplemental material.

We choose the prediction distance that leads to the best performance of the model, as output from the functions \R{tune()} and \R{perf()}\!\!.

\subsection{sparse PLS-DA}
The extension of sparse PLS to a supervised classification framework consists in coding 
the response matrix $Y$ of size $(n \times K)$ with dummy variables to indicate the class membership of each sample. Note that in this specific framework, we will \textit{only perform variable selection on the $X$ data set}, i.e., we want to select the discriminative features that can help predicting the classes of the samples. The $Y$ dummy matrix remains unchanged. Therefore, we set $M_h = X_h' Y_h$ and the optimization problem of the sPLS-DA can be written as:
$$
   \min_{a,b} ||M - ab'||^2_{F} + P_{\lambda}(a),
$$
Therefore, the penalization parameter to tune is $\lambda$.


% ===========================
% section: graphics maths
% ===========================
\section{Graphical outputs}\label{math:graphic}
In this Section, we will use the following notations: $X$ is a $ n \times p$ data matrix, and  $Y$ is a $ n \times q$ data matrix, with $n$ the number of samples (patients), $p$ and $q$ the number of variables (parameters).

\subsection{Pair-wise variable associations for CCA}
The association measure in CCA is analogous to a correlation coefficient. 
Firstly, similar to a correlation circle output, the $X^j$ and $Y^k$ variables are projected onto a lower dimensional space. Let $d\leq \min(p,q)$ the selected dimensions to adequately account for the data association, and let $z^{l}=t^{l}+u^{l}$ the equiangular vector between the canonical variates $t^{l}$ and $u^{l}$ ($l=1,\ldots ,d$), and $Z$ is a matrix containing all the $z^l$ vectors in column. The coordinates of the variable $X^j$ and $Y^k$ are obtained by projecting them on the axes defined by $z^{l}$. The projection on the $Z$ axes seems the most natural as $X$ and $Y$ are symmetrically analysed in CCA. \citet{saporta06}
showed that the $Z$ vectors have the property to be the closest to $X$ and $Y$, i.e. the sum of their squared multiple correlation coefficients with $X$ and with $Y$ is maximal. \\

\noindent
Let $\mathbf{\tilde{x}}^j=(x_{1}^j,\ldots ,x_{d}^j)'$ and $\mathbf{\tilde{y}}^k=(y_{1}^k,\ldots ,y_{d}^k)'$ the coordinates of the variable $X^j$ and $Y^k$ respectively on the axes defined by $z^{1},\ldots ,z^{d}$. These coordinates are obtained by computing the scalar innerproduct $x_{l}^j=\left\langle X^j,z^{l}\right\rangle$ and $y_{l}^k=\left\langle Y^k,z^{l}\right\rangle$ ($l=1,\ldots ,d$). As the variables $X^j$ and $Y^k$ are assumed to be of unit variance, the innerproduct is equal to the correlation between the variables $X$ (or $Y$) and $Z$: $x_{l}^j=\textrm{cor}(X^j,z^{l})$ and $y_{l}^k=\textrm{cor}(Y^k,z^{l})$. \\

\noindent
Then, for any two variables $X^j$ and $Y^k$, a similarity score can be computed as follows:
\begin{equation}
M_k^j=\langle \mathbf{\tilde{x}}^j,\mathbf{\tilde{y}}^k\rangle=(\mathbf{\tilde{x}}^j)'\mathbf{\tilde{y}}^k\, ,
\label{M_ac}
\end{equation}

\noindent where $0\leq |M_j^k|\leq 1$. The matrix $M$ can be factorized as $M=\mathbf{x}\mathbf{y}'$ with $\mathbf{x}$ and $\mathbf{y}$ matrices of order $(p\times d)$ and $(q\times d)$ respectively. When $d=2$, $M$ is represented in the correlation circle by plotting the rows of $\mathbf{x}$ and the rows of $\mathbf{y}$ as vectors in a $2$-dimensional Cartesian coordinate system. Therefore, the innerproduct of the $X^j$ and $Y^k$ coordinates is an approximation of their association score.


\subsection{Pair-wise variable associations for PLS}\label{math:pairwise} 
For PLS regression mode, the association score $M_k^j$ between the variables $X^j$ and $Y^k$ can be obtained from an approximation of their correlation coefficient. Let $r$ the rank of the matrix $X$, PLS regression mode allows for the decomposition of $X$ and $Y$ by %\cite{tenenhaus95}:
\begin{eqnarray}
  &&X=t^1(\phi^1)'+t^2(\phi^2)'+\cdots +t^r(\phi^r)'\label{dec_x}\\ \nonumber \\
  &&Y=t^1(\varphi^1)'+t^2(\varphi^2)'+\cdots +t^r(\varphi^r)'+E^{(r)}\label{dec_y}
\end{eqnarray}

\noindent where $\phi^l$ and $\varphi^l$, are the regression coefficients on the variates $t^1,\ldots ,t^r$, and $E^{(r)}$ is the residual matrix ($l=1,\ldots ,r$). By denoting $\xi_l$ the standard deviation of $t^l$, using the orthogonal properties of the variates and the decompositions in (\ref{dec_x}) and (\ref{dec_y}), we obtain $h_{l}^j=\textrm{cor}(X^j,t^{l})=\xi_l\phi_j^l$ and $g_{l}^k=\textrm{cor}(Y^k,t^{l})=\xi_l\varphi_k^l$. Let $s<r$ the number of components selected to adequately account for the variable association, then for any two variables $X^j$ and $Y^k$, the similarity score is defined by:
\begin{equation}
M_k^j=\langle h^j,g^k\rangle=\sum_{l=1}^s h_{l}^j g_{l}^k=\sum_{l=1}^s \xi_l^2 \phi_j^l \varphi_k^l \approx\textrm{cor}(X^j,Y^k)\, ,
\label{M_pls}
\end{equation}
\noindent
where $h^j=(h_{1}^j,\ldots ,h_{s}^j)'$ and $g^k=(g_{1}^k,\ldots ,g_{s}^k)'$ are the coordinates of the variable $X^j$ and $Y^k$ respectively on the axes defined by $t^{1},\ldots ,t^{s}$. When $s=2$, a correlation circle representation is obtained by plotting $h^j$ and $g^k$ as points in a 2-dimensional Cartesian coordinate system. \\

For PLS canonical mode, the association score $M_k^j$ is calculated by substituting $g_{l}^k=\textrm{cor}(Y^k,u^{l})$ in (\ref{M_pls}) for $l=1,\ldots ,s$, as in this case the decomposition of $Y$ is given by: 
$$Y=u^1(\varphi^1)'+u^2(\varphi^2)'+\cdots +u^r(\varphi^r)'+E^{(r)}$$
where $\varphi^l$ ($l=1,\ldots ,r$), are the regression coefficients on the variates $u^1,\ldots ,u^r$. Then, 
$$\textrm{cor}(X^j,Y^k)\approx\sum_{l=1}^s \xi_l\sigma_l\phi_j^l\varphi_k^l=M_k^j$$
\noindent
where $\sigma_l$ the standard deviation of $u^l$.


\subsection{Constructing Relevance Networks}
% A conceptually simple approach for modelling net-like correlation structures between two data sets is to use \textit{Relevance Networks}. This concept was introduced by \citet{butte00a} as a tool to study associations between couples of variables coming from several types of genomic data. This method generates a graph where nodes represent variables, and edges represent variable associations. The Relevance Network is built in the following simple manner. First, the correlation matrix is inferred from the data. Second, for every estimated correlation coefficients exceeding a prespecified threshold between two variables (say 0.6 in our examples), an edge is drawn between these two variables.
% 
% The construction of biological networks (gene-gene, protein-protein, etc.) with direct interactions within a variable set is of considerable interest amongst biologists, and has been extensively used in the literature. Therefore, we will not consider this case and rather focus on the representation between $X$ and $Y$ data sets, i.e., the representation of variables of two different types. We will thus display rCCA, sPLS-can and sPLS-reg Relevance Networks through the use of bipartite graph (or bigraph), that is, every node of one variable set $X$ is connected to nodes of the other variables set $Y$ only. 

The bipartite networks are inferred using the pair-wise association matrix $M$ defined in (\ref{M_ac}) and (\ref{M_pls}) for CCA and PLS results respectively. Entry $M_k^j$ in the matrix $M$ represents the association score between $X^j$ and $Y^k$ variables. Then, by setting a user-defined score threshold, the pairs of variables $X^j$ and $Y^k$ with a $|M_k^j|$ value greater than the threshold will be aggregated in the Relevance Network. By changing this threshold, the user can choose to include or exclude relationships in the Relevance Network. This option is proposed in an interactive manner in \R{mixOmics}. \\

Relevance networks for rCCA assume that the underlying network is fully connected, i.e. that there is an edge between any pair of $X$ and $Y$ variables. For sPLS, relevance networks only represent the variables selected by the model. In that case, $M_k^j$ pair-wise associations are calculated based on the selected variables. 

\subsection{Displaying  CIM}
The CIM representation is based on the pair-wise similarity matrix $M$ defined in (\ref{M_ac}) and in (\ref{M_pls}) for CCA and PLS respectively.
%on a hierarchical clustering simultaneously operating on the rows and columns of a real-valued similarity matrix $M$. The initial matrix is graphically represented as a 2-dimensional colored image, where each entry of the matrix is colored on the basis of its value, and where the rows and columns are reordered according to a hierarchical clustering. Dendrograms resulting of the clustering are added to the left (or right) side and to the top (or bottom) of the image. With rCCA, sPLS-can and sPLS-reg, we chose to display CIM based



% ===========================
% section: multiblock maths
% ===========================
\section{$N-$integration}\label{math:Ninteg}

%The deflation performed in the whole $N$-integration framework is set internally to `canonical' (a specific deflation step also performed in PLS see Chapter \ref{method:PLS}). 
\subsection{$N-$integration methods and input}
\begin{enumerate} \itemsep0em
	\item \R{block.splsda()} extends sPLS-DA and is called \textbf{DIABLO} in our manuscript \citealt{Sin16}
		\begin{itemize} \itemsep0em
		\item Outcome factor $Y$
		\item Input lasso parameter \argu{keepX}
	\end{itemize}
	\item \R{wrapper.rgcca()} extends rCCA (unsupervised, \citealt{Ten11}), improved from the RGCCA package.
	\begin{itemize} \itemsep0em
		\item Input regularization parameter \argu{tau}
	\end{itemize}
	
	\item \R{wrapper.sgcca()} extends rCCA with variable selection (unsupervised, \citealt{Ten14}), improved from the RGCCA package.
	\begin{itemize}
		\item Input lasso parameter \argu{keepX}
	\end{itemize}
	
	\item \R{block.spls()} extends sPLS.
		\begin{itemize} \itemsep0em
		\item Continuous variable / matrix $Y$ as response
		\item Input lasso parameter \argu{keepX, keepY}
	\end{itemize}
\end{enumerate}

\subsection{regularized GCCA}

The rGCCA optimization problem to solve is:
\begin{equation}\label{eq:rgcca}
\max_{\boldsymbol{a}^1, \dots, \boldsymbol{a}^J} \sum_{j, k=1, j \neq k}^{J} c_{kj}g(\mbox{Cov}(\boldsymbol{X}_j\boldsymbol{a}^j, \boldsymbol{X}_k\boldsymbol{a}^k)) \newline
\quad \mbox{subject to} \quad   \tau_j ||\boldsymbol{a}^j||^2 + (1 - \tau_j) \mbox{Var}(\boldsymbol{X}_j\boldsymbol{a}^j) = 1
\end{equation}
with $j = 1, \dots, J$ and $\boldsymbol{a}^j$ are the loading vectors associated with each block $j$. The function $g$ can be defined as:
\begin{itemize} \itemsep0em
  \item $g(x) = x$ is the Horst scheme. This is what is used in the classical PLS / PLS-DA in \R{mixOmics}
  \item $g(x) = |x|$ is the centroid scheme, which is the scheme we illustrate in this case study,
  \item $g(x) = x^2$ is the factorial scheme.
\end{itemize}
The Horst scheme requires a positive correlation between linear combinations of pairwise data sets, while the centroid and the factorial scheme enable a negative correlation between the components. In practice, we have found that the \R{scheme = 'horst'} gave satisfactory results and this is set up as a default parameter in our methods. \\

\noindent
The \textbf{regularization parameters} \R{tau} on each block $j$ $(\tau_1, \tau_2, \dots, \tau_J)$ is internally estimated from the rGCCA method using shrinkage formula from \citet{Sch05} (also used in rCCA, not covered in this workshop). Those parameters enable to numerically inverse large variance-covariance matrices. %We use the same method that was presented for rCCA (see Chapter \ref{method:CCA}).

\paragraph{Remark.} The function to call is \R{wrapper.rgcca()}, see\R{?wrapper.rgcca} for some examples. %Note that the function name is likely to change in the near future.

\subsection{sparse GCCA}
In the same vein as sparse PLS \citep{Lec08}, sparse GCCA includes Lasso penalisations on the loading vectors $\boldsymbol{a}^j$ associated to each data block to perform variable selection.  In practice we will specify the arguments \argu{keepX, keepY} in the function \R{block.spls()} to indicate the number of variables to retain in each block and for each component. \\
Using the same notations as in Equation \ref{eq:rgcca}, the optimization problem to solve is:
\begin{equation}\nonumber
\max_{\boldsymbol{a}^1, \dots, \boldsymbol{a}^J} \sum_{j, k=1, j \neq k}^{J} c_{kj}g(\mbox{Cov}(\boldsymbol{X}_j\boldsymbol{a}^j, \boldsymbol{X}_k\boldsymbol{a}^k)) \newline
\quad \mbox{subject to} \quad   ||\boldsymbol{a}^j||_2 = 1 \quad \mbox{and} \quad  ||\boldsymbol{a}^j||_1 \leq \lambda_j
\end{equation}
where $\lambda_j$ is the Lasso penalization on each block $j$. There is a direct correspondence between the Lasso parameter value and the number of variables to select in the arguments \argu{keepX, keepY}.

\paragraph{Remark.}  In the sparse GCCA, we do not need any regularization parameters \argu{tau} as the model is parsimonious (sparse). We will not give any example on \R{?block.spls()} as this method is still in development.
%\paragraph{Remark 2.} The Lasso penalty \argu{penalty} is a value between 0 (no variable selection) and 1 (all variables selected) that is used instead of the sPLS parameter \R{keepX} for convergence reasons \footnote{In a future update we will be able to provide a function with \R{keepX} instead of \R{penalty}}.



% ------------------ references -----------------
\bibliographystyle{natbib}
\bibliography{mybib}


\end{document}
